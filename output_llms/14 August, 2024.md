### Enhancing Visual Question Answering through Ranking-Based Hybrid Training andMultimodalFusion
- **标题**: 通过基于排名的混合训练和多模式融合增强视觉问答
- **Categories**: cs.CV cs.CL cs.LG
- **摘要**: 视觉问答（VQA）是一项具有挑战性的任务，要求系统根据图像内容提供准确的问题答案。由于有效捕获和集成多模态信息的局限性，当前的 VQA 模型难以解决复杂的问题。为了应对这些挑战，我们提出了 Rank VQA 模型，该模型利用排名启发的混合训练策略来提高 VQA 性能。 Rank VQA 模型集成了使用 Faster R-CNN 模型提取的高质量视觉特征和从预训练的 BERT 模型获得的丰富语义文本特征。这些特征通过采用多头自注意力机制的复杂多模态融合技术来融合。此外，还加入了排名学习模块，优化答案的相对排名，从而提高答案的准确性。混合训练策略结合了分类和排名损失，增强了模型在不同数据集上的泛化能力和鲁棒性。实验结果证明了Rank VQA模型的有效性。我们的模型在准确性和平均倒数排名 (MRR) 方面显着优于标准 VQA 数据集（包括 VQA v2.0 和 COCO-QA）上现有的最先进模型。 Rank VQA 的卓越性能体现在它处理复杂问题的能力，这些问题需要理解细微的细节并从图像和文本中做出复杂的推断。这项工作强调了基于排名的混合训练策略在提高 VQA 性能方面的有效性，并为多模式学习方法的进一步研究奠定了基础。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07303
- **Authors**: Peiyuan Chen,Zecheng Zhang,Yiping Dong,Li Zhou,Han Wang
- **Abstract**: Visual Question Answering (VQA) is a challenging task that requires systems to provide accurate answers to questions based on image content. Current VQAmodelsstruggle with complex questions due to limitations in capturing and integratingmultimodalinformation effectively. To address these challenges, we propose the Rank VQAmodel, which leverages a ranking-inspired hybrid training strategy to enhance VQA performance. The Rank VQAmodelintegrates high-quality visual features extracted using the Faster R-CNNmodeland rich semantic text features obtained from a pre-trained BERTmodel. These features are fused through a sophisticatedmultimodalfusion technique employing multi-head self-attention mechanisms. Additionally, a ranking learning module is incorporated to optimize the relative ranking of answers, thus improving answer accuracy. The hybrid training strategy combines classification and ranking losses, enhancing themodel'sgeneralization ability and robustness across diverse datasets. Experimental results demonstrate the effectiveness of the Rank VQAmodel. Ourmodelsignificantly outperforms existing state-of-the-artmodelson standard VQA datasets, including VQA v2.0 and COCO-QA, in terms of both accuracy and Mean Reciprocal Rank (MRR). The superior performance of Rank VQA is evident in its ability to handle complex questions that require understanding nuanced details and making sophisticated inferences from the image and text. This work highlights the effectiveness of a ranking-based hybrid training strategy in improving VQA performance and lays the groundwork for further research inmultimodallearning methods.

### Nonlocal Attention Operator: Materializing Hidden Knowledge Towards Interpretable Physics Discovery
- **标题**: 非局部注意力算子：将隐藏知识具体化以实现可解释的物理发现
- **Categories**: cs.LG math.AP
- **摘要**: 尽管基于注意力的神经架构最近在自然语言处理（NLP）和计算机视觉（CV）等核心人工智能领域很受欢迎，但它们在复杂物理系统建模中的潜力仍未得到充分开发。物理系统中的学习问题通常被描述为发现基于函数对的几个实例在函数空间之间映射的算子。此任务经常会出现严重不适定的偏微分方程反问题。在这项工作中，我们提出了一种基于注意力机制的新型神经算子架构，我们将其称为非局部注意力算子（NAO），并探索其开发基础物理模型的能力。特别是，我们表明注意力机制相当于一个双积分算子，它能够实现空间标记之间的非局部交互，并具有一个数据相关的内核来表征从数据到底层算子的隐藏参数字段的逆映射。因此，注意力机制从多个系统生成的训练数据中提取全局先验信息，并以非线性核图的形式建议探索空间。因此，NAO 可以通过编码正则化和实现泛化性来解决逆偏微分方程问题中的不适定性和秩不足问题。我们凭经验证明了 NAO 相对于基线神经模型在对未见过的数据分辨率和系统状态的泛化性方面的优势。我们的工作不仅提出了一种新颖的神经算子架构来学习物理系统的可解释基础模型，而且还为理解注意力机制提供了新的视角。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07307
- **Authors**: Yue Yu,Ning Liu,Fei Lu,Tian Gao,Siavash Jafarzadeh,Stewart Silling
- **Abstract**: Despite the recent popularity of attention-based neural architectures in core AI fields like naturallanguageprocessing (NLP) and computer vision (CV), their potential inmodelingcomplex physical systems remains under-explored. Learning problems in physical systems are often characterized as discovering operators that map between function spaces based on a few instances of function pairs. This task frequently presents a severely ill-posed PDE inverse problem. In this work, we propose a novel neural operator architecture based on the attention mechanism, which we coin Nonlocal Attention Operator (NAO), and explore its capability towards developing a foundation physicalmodel. In particular, we show that the attention mechanism is equivalent to a double integral operator that enables nonlocal interactions among spatial tokens, with a data-dependent kernel characterizing the inverse mapping from data to the hidden parameter field of the underlying operator. As such, the attention mechanism extracts global prior information from training data generated by multiple systems, and suggests the exploratory space in the form of a nonlinear kernel map. Consequently, NAO can address ill-posedness and rank deficiency in inverse PDE problems by encoding regularization and achieving generalizability. We empirically demonstrate the advantages of NAO over baseline neuralmodelsin terms of generalizability to unseen data resolutions and system states. Our work not only suggests a novel neural operator architecture for learning interpretable foundationmodelsof physical systems, but also offers a new perspective towards understanding the attention mechanism.

### MultiSurf-GPT: Facilitating Context-Aware Reasoning with Large-ScaleLanguageModelsforMultimodalSurface Sensing
- **标题**: MultiSurf-GPT：利用大规模语言模型促进多模态表面传感的上下文感知推理
- **Categories**: cs.HC doi 10.1145/3640471.3680450
- **摘要**: 表面传感广泛应用于健康诊断、制造和安全监测。移动传感的进步为移动计算中的情境感知提供了潜力，通常采用单一传感模式。新兴的多模态大规模语言模型提供了新的机遇。我们提出了 MultiSurf-GPT，它利用 GPT-4o 的先进功能，根据提示策略（零样本和少样本提示）统一处理和解释不同模式（雷达、显微镜和多光谱数据）。我们通过使用 MultiSurf-GPT 来识别低级信息并推断高级上下文感知分析来初步验证我们的框架，展示了增强上下文感知洞察的能力。该框架有望成为未来加速开发更复杂的上下文感知应用程序的工具，提供更快、更具成本效益的集成解决方案。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07311
- **Authors**: Yongquan Hu,Black Sun,Pengcheng An,Zhuying Li,Wen Hu,Aaron J. Quigley
- **Abstract**: Surface sensing is widely employed in health diagnostics, manufacturing and safety monitoring. Advances in mobile sensing affords this potential for context awareness in mobile computing, typically with a single sensing modality. Emergingmultimodallarge-scalelanguagemodelsoffer new opportunities. We propose MultiSurf-GPT, which utilizes the advanced capabilities ofGPT-4o to process and interpret diverse modalities (radar, microscope and multispectral data) uniformly based on prompting strategies (zero-shot and few-shot prompting). We preliminarily validated our framework by using MultiSurf-GPTto identify low-level information, and to infer high-level context-aware analytics, demonstrating the capability of augmenting context-aware insights. This framework shows promise as a tool to expedite the development of more complex context-aware applications in the future, providing a faster, more cost-effective, and integrated solution.

### Exploring Large-ScaleLanguageModelsto Evaluate EEG-BasedMultimodalData for Mental Health
- **标题**: 探索大规模语言模型以评估基于脑电图的心理健康多模态数据
- **Categories**: cs.HC doi 10.1145/3675094.3678494
- **摘要**: 将脑电图（EEG）等生理信号与采访音频等其他数据相结合，可以为心理状态或神经系统疾病提供有价值的多模式见解。大型语言模型（LLM）的最新进展将其定位为心理健康评估的潜在“健康代理人”。然而，当前的研究主要集中在单一数据模式，这提供了通过多模式数据增进理解的机会。我们的研究旨在通过使用法学硕士研究用于心理健康评估的多模态数据来推进这种方法，特别是通过零样本和少样本提示。采用三个数据集结合脑电图、面部表情和音频（文本）进行抑郁和情绪分类。结果表明，在心理健康评估中，多模态信息比单一模态方法具有显着优势。值得注意的是，将脑电图与常用的法学硕士模式（例如音频和图像）相结合显示出巨大的潜力。此外，我们的研究结果表明，与零样本学习方法相比，1 样本学习提供了更大的好处。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07313
- **Authors**: Yongquan Hu,Shuning Zhang,Ting Dang,Hong Jia,Flora D. Salim,Wen Hu,Aaron J. Quigley
- **Abstract**: Integrating physiological signals such as electroencephalogram (EEG), with other data such as interview audio, may offer valuablemultimodalinsights into psychological states or neurological disorders. Recent advancements with LargeLanguageModels(LLMs) position them as prospective ``health agents'' for mental health assessment. However, current research predominantly focus on single data modalities, presenting an opportunity to advance understanding throughmultimodaldata. Our study aims to advance this approach by investigatingmultimodaldata usingLLMsfor mental health assessment, specifically through zero-shot and few-shot prompting. Three datasets are adopted for depression and emotion classifications incorporating EEG, facial expressions, and audio (text). The results indicate thatmultimodalinformation confers substantial advantages over single modality approaches in mental health assessment. Notably, integrating EEG alongside commonly usedLLMmodalities such as audio and images demonstrates promising potential. Moreover, our findings reveal that 1-shot learning offers greater benefits compared to zero-shot learning methods.

### LLM-Enhanced Static Analysis for Precise Identification of Vulnerable OSS Versions
- **标题**: LLM 增强静态分析，精确识别易受攻击的 OSS 版本
- **Categories**: cs.SE cs.CR
- **摘要**: 开源软件 (OSS) 因其协作开发模式和成本效益的特性而受到欢迎。然而，在开发项目中采用特定的软件版本，当这些版本带来漏洞时，可能会带来安全风险。当前识别易受攻击版本的方法通常使用具有预定义规则的静态分析来分析和跟踪漏洞补丁中涉及的代码。然后，他们使用语法级代码克隆检测来识别易受攻击的版本。由于 (1) 在分析中包含与漏洞无关的代码以及 (2) 语法级代码克隆检测的不足，这些方法受到不精确性的阻碍。本文介绍了 Vercation，一种旨在识别用 C/C++ 编写的 OSS 易受攻击版本的方法。 Vercation 将程序切片与 LargeLanguageModel (LLM) 相结合，以从漏洞补丁中识别与漏洞相关的代码。然后，它回溯历史提交，以收集已识别的漏洞相关代码的先前修改。我们提出语义级代码克隆检测来比较修改前和修改后代码之间的差异，从而定位引入漏洞的提交（vic）并能够识别补丁提交和vic之间的易受攻击的版本。我们整理了一个链接 74 个 OSS 漏洞和 1013 个版本的数据集来评估 Vercation。在此数据集上，我们的方法达到了 92.4% 的 F1 分数，优于当前最先进的方法。更重要的是，Vercation 在 NVD 报告中检测到 134 个不正确的易受攻击的 OSS 版本。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07321
- **Authors**: Yiran Cheng,Lwin Khin Shar,Ting Zhang,Shouguo Yang,Chaopeng Dong,David Lo,Shichao Lv,Zhiqiang Shi,Limin Sun
- **Abstract**: Open-source software (OSS) has experienced a surge in popularity, attributed to its collaborative developmentmodeland cost-effective nature. However, the adoption of specific software versions in development projects may introduce security risks when these versions bring along vulnerabilities. Current methods of identifying vulnerable versions typically analyze and trace the code involved in vulnerability patches using static analysis with pre-defined rules. They then use syntactic-level code clone detection to identify the vulnerable versions. These methods are hindered by imprecisions due to (1) the inclusion of vulnerability-irrelevant code in the analysis and (2) the inadequacy of syntactic-level code clone detection. This paper presents Vercation, an approach designed to identify vulnerable versions of OSS written in C/C++. Vercation combines program slicing with a LargeLanguageModel(LLM) to identify vulnerability-relevant code from vulnerability patches. It then backtraces historical commits to gather previous modifications of identified vulnerability-relevant code. We propose semantic-level code clone detection to compare the differences between pre-modification and post-modification code, thereby locating the vulnerability-introducing commit (vic) and enabling to identify the vulnerable versions between the patch commit and the vic. We curate a dataset linking 74 OSS vulnerabilities and 1013 versions to evaluate Vercation. On this dataset, our approach achieves the F1 score of 92.4%, outperforming current state-of-the-art methods. More importantly, Vercation detected 134 incorrect vulnerable OSS versions in NVD reports.

### LPU: A Latency-Optimized and Highly Scalable Processor for LargeLanguageModelInference
- **标题**: LPU：用于大型语言模型推理的延迟优化且高度可扩展的处理器
- **Categories**: cs.AR
- **摘要**: OpenAI ChatGPT 的爆炸性到来推动了大型语言模型（LLM）的全球化，该模型由数十亿个预训练参数组成，体现了语法和语义方面。 HyperAccel 引入了延迟处理单元 (LPU)，这是一种延迟优化且高度可扩展的处理器架构，用于加速 LLMinference。 LPU 通过简化的数据流完美平衡内存带宽和计算逻辑，以最大限度地提高性能和效率。 LPU配备可扩展同步链路（ESL），隐藏多个LPU之间的数据同步延迟。 HyperDex 作为 LPU 的补充，作为运行 LLM 应用程序的直观软件框架。 LPU 对于 1.3B 和 66B 模型分别实现 1.25 ms/token 和 20.9 ms/token，比 GPU 快 2.09 倍和 1.37 倍。 LPU采用三星4nm工艺合成，总面积为0.824 mm2，功耗为284.31 mW。基于 LPU 的服务器的能源效率分别比 NVIDIA H100 和 L4 服务器高 1.33 倍和 1.32 倍。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07326
- **Authors**: Seungjae Moon,Jung-Hoon Kim,Junsoo Kim,Seongmin Hong,Junseo Cha,Minsu Kim,Sukbin Lim,Gyubin Choi,Dongjin Seo,Jongho Kim,Hunjong Lee,Hyunjun Park,Ryeowook Ko,Soongyu Choi,Jongse Park,Jinwon Lee,Joo-Young Kim
- **Abstract**: The explosive arrival of OpenAI's ChatGPT has fueled the globalization of largelanguagemodel(LLM), which consists of billions of pretrained parameters that embodies the aspects of syntax and semantics. HyperAccel introduces latency processing unit (LPU), a latency-optimized and highly scalable processor architecture for the acceleration ofLLMinference. LPU perfectly balances the memory bandwidth and compute logic with streamlined dataflow to maximize performance and efficiency. LPU is equipped with expandable synchronization link (ESL) that hides data synchronization latency between multiple LPUs. HyperDex complements LPU as an intuitive software framework to runLLMapplications. LPU achieves 1.25 ms/token and 20.9 ms/token for 1.3B and 66Bmodel, respectively, which is 2.09x and 1.37x faster than the GPU. LPU, synthesized using Samsung 4nm process, has total area of 0.824 mm2 and power consumption of 284.31 mW. LPU-based servers achieve 1.33x and 1.32x energy efficiency over NVIDIA H100 and L4 servers, respectively.

### Robust Semi-supervisedMultimodalMedical Image Segmentation via Cross Modality Collaboration
- **标题**: 通过跨模态协作进行鲁棒半监督多模态医学图像分割
- **Categories**: cs.CV
- **摘要**: 多模态学习利用来自不同模态的互补信息，从而提高医学图像分割的性能。然而，流行的多模态学习方法严重依赖来自各种模态的大量注释良好的数据来实现准确的分割性能。由于此类数据的可用性有限，这种依赖性常常在临床环境中构成挑战。此外，不同成像模式之间固有的解剖学失准进一步使增强分割性能的努力变得复杂。为了解决这个问题，我们提出了一种新颖的半监督多模态分割框架，该框架对于稀缺的标记数据和未对齐的模态具有鲁棒性。我们的框架采用一种新颖的跨模态协作策略来提取与每种模态固有相关的独立于模态的知识，并将这些信息集成到统一的融合层中以进行特征合并。通过通道方式的语义一致性损失，我们的框架确保从跨模态的特征角度对齐独立于模态的信息，从而增强其防止多模态场景中的错位。此外，我们的框架有效地整合了对比一致学习来调节解剖结构，促进半监督分割任务中未标记数据的解剖学预测对齐。与其他多模式方法相比，我们的方法在心脏、腹部多器官和甲状腺相关眼病分割这三个任务中实现了具有竞争力的性能。它还在涉及稀缺标记数据和未对齐模式的场景中表现出出色的鲁棒性。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07341
- **Authors**: Xiaogen Zhon,Yiyou Sun,Min Deng,Winnie Chiu Wing Chu,Qi Dou
- **Abstract**: Multimodallearning leverages complementary information derived from different modalities, thereby enhancing performance in medical image segmentation. However, prevailingmultimodallearning methods heavily rely on extensive well-annotated data from various modalities to achieve accurate segmentation performance. This dependence often poses a challenge in clinical settings due to limited availability of such data. Moreover, the inherent anatomical misalignment between different imaging modalities further complicates the endeavor to enhance segmentation performance. To address this problem, we propose a novel semi-supervisedmultimodalsegmentation framework that is robust to scarce labeled data and misaligned modalities. Our framework employs a novel cross modality collaboration strategy to distill modality-independent knowledge, which is inherently associated with each modality, and integrates this information into a unified fusion layer for feature amalgamation. With a channel-wise semantic consistency loss, our framework ensures alignment of modality-independent information from a feature-wise perspective across modalities, thereby fortifying it against misalignments inmultimodalscenarios. Furthermore, our framework effectively integrates contrastive consistent learning to regulate anatomical structures, facilitating anatomical-wise prediction alignment on unlabeled data in semi-supervised segmentation tasks. Our method achieves competitive performance compared to othermultimodalmethods across three tasks: cardiac, abdominal multi-organ, and thyroid-associated orbitopathy segmentations. It also demonstrates outstanding robustness in scenarios involving scarce labeled data and misaligned modalities.

### DoGPTLanguageModelsSuffer From Split Personality Disorder? The Advent Of Substrate-Free Psychometrics
- **标题**: GPT语言模型是否患有人格分裂症？无基质心理测量学的出现
- **Categories**: cs.CL cs.AI cs.CY doi 10.21203/rs.3.rs-2717108/v1
- **摘要**: 先前对大型语言模型的出现的研究表明，这些模型表现出明显的类人能力和心理潜在特征。然而，结果在这些潜在特征的表达和程度方面存在部分矛盾，但一致认为在自恋、精神病和马基雅维利主义的黑暗三人格上得分高的令人担忧的倾向，再加上出轨的记录，需要更严格的研究关于这些模型的安全性。我们提供了最先进的语言模型，以及九种语言的相同人格问卷，并对高斯混合模型进行了贝叶斯分析，为更深层次的问题找到了证据。我们的结果表明语间和语内的不稳定性，这表明当前的语言模型没有形成一致的核心人格。这可能会导致基于这些基础模型并日益融入人类生活的人工智能系统出现不安全行为。随后，我们讨论了现代心理测量学的缺点，对其进行了抽象，并为其物种中立、无基质的表述提供了一个框架。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07377
- **Authors**: Peter Romero,Stephen Fitz,Teruo Nakatsuma
- **Abstract**: Previous research on emergence in largelanguagemodelsshows these display apparent human-like abilities and psychological latent traits. However, results are partly contradicting in expression and magnitude of these latent traits, yet agree on the worrisome tendencies to score high on the Dark Triad of narcissism, psychopathy, and Machiavellianism, which, together with a track record of derailments, demands more rigorous research on safety of thesemodels. We provided a state of the artlanguagemodelwith the same personality questionnaire in ninelanguages, and performed Bayesian analysis of Gaussian MixtureModel, finding evidence for a deeper-rooted issue. Our results suggest both interlingual and intralingual instabilities, which indicate that currentlanguagemodelsdo not develop a consistent core personality. This can lead to unsafe behaviour of artificial intelligence systems that are based on these foundationmodels, and are increasingly integrated in human life. We subsequently discuss the shortcomings of modern psychometrics, abstract it, and provide a framework for its species-neutral, substrate-free formulation.

### DataVisT5: A Pre-trainedLanguageModelfor Jointly Understanding Text and Data Visualization
- **标题**: DataVisT5：用于联合理解文本和数据可视化的预训练语言模型
- **Categories**: cs.CL cs.AI cs.DB
- **摘要**: 数据可视化（DV）是提高传达大数据背后的见解的效率的基础和前提工具，在现有的数据驱动的世界中已被广泛接受。 DV 中的任务自动化，例如将自然语言查询转换为可视化（即文本到可视化）、从可视化生成解释（即可视化到文本）、以自由形式回答与 DV 相关的问题（即 FeVisQA）以及解释表格数据（即表格到文本）对于推进该领域至关重要。尽管 T5 和 BERT 等预训练语言模型 (PLM) 具有潜力，但其在 DV 中的应用受到高成本和处理跨模态信息挑战的限制，导致针对 DV 的 PLM 的研究很少。我们引入了 \textbf{DataVisT5}，这是一种专为 DV 量身定制的新型 PLM，它通过混合目标预训练和多任务微调策略来增强 T5 架构，集成文本和 DV 数据集以有效解释跨模态语义。对公共数据集的广泛评估表明，DataVisT5 在各种 DV 相关任务上始终优于当前最先进的模型。我们预计DataVisT5不仅会激发对垂直PLM的进一步研究，还会扩大PLM的应用范围。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07401
- **Authors**: Zhuoyue Wan,Yuanfeng Song,Shuaimin Li,Chen Jason Zhang,Raymond Chi-Wing Wong
- **Abstract**: Data visualization (DV) is the fundamental and premise tool to improve the efficiency in conveying the insights behind the big data, which has been widely accepted in existing data-driven world. Task automation in DV, such as converting naturallanguagequeries to visualizations (i.e., text-to-vis), generating explanations from visualizations (i.e., vis-to-text), answering DV-related questions in free form (i.e. FeVisQA), and explicating tabular data (i.e., table-to-text), is vital for advancing the field. Despite their potential, the application of pre-trainedlanguagemodels(PLMs) like T5 and BERT in DV has been limited by high costs and challenges in handling cross-modal information, leading to few studies on PLMs for DV. We introduce \textbf{DataVisT5}, a novel PLM tailored for DV that enhances the T5 architecture through a hybrid objective pre-training and multi-task fine-tuning strategy, integrating text and DV datasets to effectively interpret cross-modal semantics. Extensive evaluations on public datasets show that DataVisT5 consistently outperforms current state-of-the-artmodelson various DV-related tasks. We anticipate that DataVisT5 will not only inspire further research on vertical PLMs but also expand the range of applications for PLMs.

### A Quantum-Inspired Analysis of Human Disambiguation Processes
- **标题**: 人类消歧过程的量子启发分析
- **Categories**: cs.CL cs.AI cs.LO quant-ph
- **摘要**: 形式语言对于计算机编程至关重要，并且被构建为易于计算机处理。相比之下，自然语言更具挑战性，并催生了自然语言处理（NLP）领域。一个主要障碍是普遍存在的模糊性。 NLP 的最新进展促进了大型语言模型的发展，可以高精度地解决歧义问题。与此同时，量子计算机近年来因其能够比经典计算机更快地解决一些计算问题而受到广泛关注。这种新的计算范式已经进入机器学习和自然语言处理领域，其中出现了混合经典量子学习算法。然而，还需要更多的研究来确定哪些 NLP 任务可以从真正的量子优势中受益。在本论文中，我们应用基础量子力学中产生的形式主义（例如上下文和因果关系）来研究语言学中产生的歧义。通过这样做，我们还重现了与人类消歧过程相关的心理语言学结果。这些结果随后被用于预测人类行为，并且优于当前的 NLP 方法。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07402
- **Authors**: Daphne Wang
- **Abstract**: Formallanguagesare essential for computer programming and are constructed to be easily processed by computers. In contrast, naturallanguagesare much more challenging and instigated the field of NaturalLanguageProcessing (NLP). One major obstacle is the ubiquity of ambiguities. Recent advances in NLP have led to the development of largelanguagemodels, which can resolve ambiguities with high accuracy. At the same time, quantum computers have gained much attention in recent years as they can solve some computational problems faster than classical computers. This new computing paradigm has reached the fields of machine learning and NLP, where hybrid classical-quantum learning algorithms have emerged. However, more research is needed to identify which NLP tasks could benefit from a genuine quantum advantage. In this thesis, we applied formalisms arising from foundational quantum mechanics, such as contextuality and causality, to study ambiguities arising from linguistics. By doing so, we also reproduced psycholinguistic results relating to the human disambiguation process. These results were subsequently used to predict human behaviour and outperformed current NLP methods.

### Aquila2 Technical Report
- **标题**: Aquila2 技术报告
- **Categories**: cs.CL
- **摘要**: 本文介绍了 Aquila2 系列，该系列包含参数大小为 7、34 和 700 亿的多种双语模型。这些模型基于名为 HeuriMentor (HM) 的创新框架进行训练，该框架提供对模型收敛的实时洞察并增强训练过程和数据管理。 HM系统由自适应训练引擎（ATE）、训练状态监视器（TSM）和数据管理单元（DMU）组成，可以精确监控模型的训练进度，并能够有效优化数据分布，从而提高训练效果。广泛的评估表明，Aquila2 模型系列在英语和中文基准测试中表现相当出色。具体来说，Aquila2-34B 在量化为 Int4 时仅表现出轻微的性能下降。此外，我们还公开了我们的训练代码 (https://github.com/FlagOpen/FlagScale) 和模型权重 (https://github.com/FlagAI-Open/Aquila2)，以支持正在进行的研究和应用程序的开发。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07410
- **Authors**: Bo-Wen Zhang,Liangdong Wang,Jijie Li,Shuhao Gu,Xinya Wu,Zhengduo Zhang,Boyan Gao,Yulong Ao,Guang Liu
- **Abstract**: This paper introduces the Aquila2 series, which comprises a wide range of bilingualmodelswith parameter sizes of 7, 34, and 70 billion. Thesemodelsare trained based on an innovative framework named HeuriMentor (HM), which offers real-time insights intomodelconvergence and enhances the training process and data management. The HM System, comprising the Adaptive Training Engine (ATE), Training State Monitor (TSM), and Data Management Unit (DMU), allows for precise monitoring of themodel'straining progress and enables efficient optimization of data distribution, thereby enhancing training effectiveness. Extensive evaluations show that the Aquila2modelseries performs comparably well on both English and Chinese benchmarks. Specifically, Aquila2-34B demonstrates only a slight decrease in performance when quantized to Int4. Furthermore, we have made our training code (https://github.com/FlagOpen/FlagScale) andmodelweights (https://github.com/FlagAI-Open/Aquila2) publicly available to support ongoing research and the development of applications.

### Knowledge in Superposition: Unveiling the Failures of Lifelong Knowledge Editing for LargeLanguageModels
- **标题**: 知识叠加：揭示大型语言模型终身知识编辑的失败
- **Categories**: cs.CL
- **摘要**: 知识编辑旨在更新大型语言模型（LLM）中过时或不正确的知识。然而，当前的知识编辑方法对于终身编辑的可扩展性有限。本研究探讨了知识编辑在终身编辑中失败的根本原因。我们从源自线性联想记忆的封闭式解决方案开始，它支撑着最先进的知识编辑方法。我们将解决方案从单一编辑扩展到终身编辑，并通过严格的数学推导，识别出最终解决方案中的干扰项，表明编辑知识可能会影响不相关的知识。对干扰项的进一步分析揭示了知识表示之间的叠加关系。当语言模型中不存在知识叠加时，干扰项消失，从而实现无损知识编辑。跨多种语言模型的实验表明，知识叠加是普遍的，表现出高峰度、零均值和重尾分布，具有清晰的标度规律。最终，我们通过理论与实验相结合，论证了知识叠加是终身编辑失败的根本原因。此外，这是第一项从叠加角度研究知识编辑的研究，并提供了对众多现实世界语言模型的叠加的全面观察。代码可在 https://github.com/ChenhuiHu/knowledge_in_superposition 获取。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07413
- **Authors**: Chenhui Hu,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao
- **Abstract**: Knowledge editing aims to update outdated or incorrect knowledge in largelanguagemodels(LLMs). However, current knowledge editing methods have limited scalability for lifelong editing. This study explores the fundamental reason why knowledge editing fails in lifelong editing. We begin with the closed-form solution derived from linear associative memory, which underpins state-of-the-art knowledge editing methods. We extend the solution from single editing to lifelong editing, and through rigorous mathematical derivation, identify an interference term in the final solution, suggesting that editing knowledge may impact irrelevant knowledge. Further analysis of the interference term reveals a close relationship with superposition between knowledge representations. When knowledge superposition does not exist inlanguagemodels, the interference term vanishes, allowing for lossless knowledge editing. Experiments across numerouslanguagemodelsreveal that knowledge superposition is universal, exhibiting high kurtosis, zero mean, and heavy-tailed distributions with clear scaling laws. Ultimately, by combining theory and experiments, we demonstrate that knowledge superposition is the fundamental reason for the failure of lifelong editing. Moreover, this is the first study to investigate knowledge editing from the perspective of superposition and provides a comprehensive observation of superposition across numerous real-worldlanguagemodels. Code available at https://github.com/ChenhuiHu/knowledge_in_superposition.

### Rethinking Open-Vocabulary Segmentation of Radiance Fields in 3D Space
- **标题**: 重新思考 3D 空间中辐射场的开放词汇分割
- **Categories**: cs.CV
- **摘要**: 理解场景的 3D 语义是各种场景（例如实体代理）的基本问题。虽然 NeRF 和 3DGS 擅长新颖视图合成，但以前理解其语义的方法仅限于不完整的 3D 理解：它们的分割结果是 2D 掩模，并且它们的监督锚定于 2D 像素。本文重新审视该问题集，以更好地理解由 NeRF 和 3DGS 建模的场景，如下所示。 1）我们直接监督3D点来训练语言嵌入场。它无需依赖多尺度语言嵌入即可实现最先进的准确性。 2）我们将预训练的语言字段转移到3DGS，在不牺牲训练时间或准确性的情况下实现了第一实时渲染速度。 3）我们引入了一种 3D 查询和评估协议，用于一起评估重建的几何和语义。代码、检查点和注释将在线提供。项目页面：https://hyunji12.github.io/Open3DRF
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07416
- **Authors**: Hyunjee Lee,Youngsik Yun,Jeongmin Bae,Seoha Kim,Youngjung Uh
- **Abstract**: Understanding the 3D semantics of a scene is a fundamental problem for various scenarios such as embodied agents. While NeRFs and 3DGS excel at novel-view synthesis, previous methods for understanding their semantics have been limited to incomplete 3D understanding: their segmentation results are 2D masks and their supervision is anchored at 2D pixels. This paper revisits the problem set to pursue a better 3D understanding of a scenemodeledby NeRFs and 3DGS as follows. 1) We directly supervise the 3D points to train thelanguageembedding field. It achieves state-of-the-art accuracy without relying on multi-scalelanguageembeddings. 2) We transfer the pre-trainedlanguagefield to 3DGS, achieving the first real-time rendering speed without sacrificing training time or accuracy. 3) We introduce a 3D querying and evaluation protocol for assessing the reconstructed geometry and semantics together. Code, checkpoints, and annotations will be available online. Project page: https://hyunji12.github.io/Open3DRF

### LLMI3D: EmpoweringLLMwith 3D Perception from a Single 2D Image
- **标题**: LLMI3D：通过单个 2D 图像的 3D 感知增强LLM的能力
- **Categories**: cs.CV cs.AI
- **摘要**: 自动驾驶、增强现实、机器人技术和实体智能的最新进展使得 3D 感知算法成为必要。然而，当前的 3D 感知方法，尤其是小型模型，在处理逻辑推理、问题回答和处理开放场景类别方面存在困难。另一方面，生成式多模态大语言模型（MLLM）在一般能力方面表现出色，但在 3D 任务中表现不佳，因为空间和局部物体感知较弱，基于文本的几何数值输出较差，并且无法处理相机焦距变化。为了应对这些挑战，我们提出了以下解决方案：用于更好地提取空间特征的空间增强局部特征挖掘、用于精确几何回归的 3D 查询令牌派生信息解码以及用于处理相机焦距变化的基于几何投影的 3D 推理。我们对预训练的 MLLM 采用参数高效的微调，并开发了 LLMI3D，这是一种强大的 3D 感知 MLLM。此外，我们还构建了 IG3D 数据集，它提供了细粒度的描述和问答注释。大量实验表明，我们的 LLMI3D 实现了最先进的性能，显着优于现有方法。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07422
- **Authors**: Fan Yang,Sicheng Zhao,Yanhao Zhang,Haoxiang Chen,Hui Chen,Wenbo Tang,Haonan Lu,Pengfei Xu,Zhenyu Yang,Jungong Han,Guiguang Ding
- **Abstract**: Recent advancements in autonomous driving, augmented reality, robotics, and embodied intelligence have necessitated 3D perception algorithms. However, current 3D perception methods, particularly smallmodels, struggle with processing logical reasoning, question-answering, and handling open scenario categories. On the other hand, generativemultimodallargelanguagemodels(MLLMs) excel in general capacity but underperform in 3D tasks, due to weak spatial and local object perception, poor text-based geometric numerical output, and inability to handle camera focal variations. To address these challenges, we propose the following solutions: Spatial-Enhanced Local Feature Mining for better spatial feature extraction, 3D Query Token-Derived Info Decoding for precise geometric regression, and Geometry Projection-Based 3D Reasoning for handling camera focal length variations. We employ parameter-efficient fine-tuning for a pre-trained MLLM and develop LLMI3D, a powerful 3D perception MLLM. Additionally, we have constructed the IG3D dataset, which provides fine-grained descriptions and question-answer annotations. Extensive experiments demonstrate that our LLMI3D achieves state-of-the-art performance, significantly outperforming existing methods.

### Exploring Retrieval Augmented Generation in Arabic
- **标题**: 探索阿拉伯语检索增强生成
- **Categories**: cs.CL cs.AI
- **摘要**: 最近，检索增强生成（RAG）已成为自然语言处理领域的一项强大技术，它结合了基于检索和基于生成的模型的优势来增强文本生成任务。然而，RAG 在阿拉伯语这种具有独特特征和资源限制的语言中的应用仍然有待探索。本文提出了关于阿拉伯语文本 RAG 的实施和评估的综合案例研究。这项工作的重点是探索检索阶段的各种语义嵌入模型和生成阶段的几个法学硕士，以研究在阿拉伯语语境中哪些有效，哪些无效。这项工作还涉及检索阶段文档方言和查询方言之间的差异问题。结果表明，现有的语义嵌入模型和LLM可以有效地用于构建阿拉伯语RAG管道。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07425
- **Authors**: Samhaa R. El-Beltagy,Mohamed A. Abdallah
- **Abstract**: Recently, Retrieval Augmented Generation (RAG) has emerged as a powerful technique in naturallanguageprocessing, combining the strengths of retrieval-based and generation-basedmodelsto enhance text generation tasks. However, the application of RAG in Arabic, alanguagewith unique characteristics and resource constraints, remains underexplored. This paper presents a comprehensive case study on the implementation and evaluation of RAG for Arabic text. The work focuses on exploring various semantic embeddingmodelsin the retrieval stage and severalLLMsin the generation stage, in order to investigate what works and what doesn't in the context of Arabic. The work also touches upon the issue of variations between document dialect and query dialect in the retrieval stage. Results show that existing semantic embeddingmodelsandLLMscan be effectively employed to build Arabic RAG pipelines.

### Beyond Inter-Item Relations: Dynamic Adaptive Mixture-of-Experts forLLM-Based Sequential Recommendation
- **标题**: 超越项目间关系：基于LLM的顺序推荐的动态自适应专家混合
- **Categories**: cs.IR
- **摘要**: 顺序推荐系统（SRS）根据用户历史交互序列预测用户可能喜欢的下一个项目。受各种人工智能应用中大型语言模型 (LLM) 兴起的启发，基于 LLM 的 SRS 的工作激增。尽管它们的性能很有吸引力，但现有的基于LLM的SRS仍然表现出一些局限性，包括忽略项内关系、忽略长期协作知识以及使用不灵活的架构设计来进行适应。为了缓解这些问题，我们提出了一种名为 MixRec 的基于 LLM 的 SRS。 MixRec 建立在用于捕获项目间关系的粗粒度自适应之上，通过以下方式进一步增强：(1) 对项目内关系进行建模的上下文屏蔽，以帮助LLM更好地理解 SRS 上下文中的标记和项目语义，(2) 帮助 LLMin 企业进行协作知识注入长期协作知识；（3）动态自适应专家混合设计，可以灵活选择基于贝叶斯优化的专家架构，以更好地融合不同的顺序信息。大量的实验表明，MixRec 可以以动态和自适应的方式有效地处理顺序推荐。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07427
- **Authors**: CanYi Liu,Wei Li,Youchen,Zhang,Hui Li,Rongrong Ji
- **Abstract**: Sequential recommender system (SRS) predicts the next items that users may prefer based on user historical interaction sequences. Inspired by the rise of largelanguagemodels(LLMs) in various AI applications, there is a surge of work onLLM-based SRS. Despite their attractive performance, existingLLM-based SRS still exhibit some limitations, including neglecting intra-item relations, ignoring long-term collaborative knowledge and using inflexible architecture designs for adaption. To alleviate these issues, we propose anLLM-based SRS named MixRec. Built on top of coarse-grained adaption for capturing inter-item relations, MixRec is further enhanced with (1) context masking thatmodelsintra-item relations to helpLLMbetter understand token and item semantics in the context of SRS, (2) collaborative knowledge injection that helpsLLMincorporate long-term collaborative knowledge, and (3) a dynamic adaptive mixture-of-experts design that can flexibly choose expert architectures based on Bayesian optimization to better incorporate different sequential information. Extensive experiments demonstrate that MixRec can effectively handle sequential recommendation in a dynamic and adaptive manner.

### Modality InvariantMultimodalLearning to Handle Missing Modalities: A Single-Branch Approach
- **标题**: 模态不变多模态学习处理缺失模态：单分支方法
- **Categories**: cs.CV
- **摘要**: 多模态网络比单模态网络表现出显着的性能改进。现有的多模态网络是以多分支方式设计的，由于对融合策略的依赖，如果缺少一种或多种模态，就会表现出性能下降。在这项工作中，我们提出了一种模态不变的多模态学习方法，该方法不易受到缺失模态的影响。它由一个单分支网络组成，该网络在多种模态之间共享权重，以学习模态间表示，从而最大限度地提高性能以及对缺失模态的鲁棒性。在四个具有挑战性的数据集上进行了广泛的实验，包括文本视觉（UPMC Food-101、Hateful Memes、Ferramenta）和视听模式（VoxCeleb1）。与现有的最先进方法相比，我们提出的方法在所有模态都存在以及在训练或测试期间缺少模态的情况下实现了卓越的性能。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07445
- **Authors**: Muhammad Saad Saeed,Shah Nawaz,Muhammad Zaigham Zaheer,Muhammad Haris Khan,Karthik Nandakumar,Muhammad Haroon Yousaf,Hassan Sajjad,Tom De Schepper,Markus Schedl
- **Abstract**: Multimodalnetworks have demonstrated remarkable performance improvements over their unimodal counterparts. Existingmultimodalnetworks are designed in a multi-branch fashion that, due to the reliance on fusion strategies, exhibit deteriorated performance if one or more modalities are missing. In this work, we propose a modality invariantmultimodallearning method, which is less susceptible to the impact of missing modalities. It consists of a single-branch network sharing weights across multiple modalities to learn inter-modality representations to maximize performance as well as robustness to missing modalities. Extensive experiments are performed on four challenging datasets including textual-visual (UPMC Food-101, Hateful Memes, Ferramenta) and audio-visual modalities (VoxCeleb1). Our proposed method achieves superior performance when all modalities are present as well as in the case of missing modalities during training or testing compared to the existing state-of-the-art methods.

### CMU's IWSLT 2024 Simultaneous Speech Translation System
- **标题**: CMU 的 IWSLT 2024 同步语音翻译系统
- **Categories**: cs.CL cs.AI
- **摘要**: 本文介绍了卡耐基梅隆大学向 IWSLT 2024 同步语音翻译 (SST) 任务提交的内容，该任务以流式传输方式将英语语音翻译为德语文本。我们的端到端语音转文本 (ST) 系统集成了 WavLM 语音编码器、模态适配器和作为解码器的 Llama2-7B-Base 模型。我们采用两阶段训练方法：首先，我们对齐语音和文本的表示，然后进行全面微调。这两个阶段都在具有交叉熵损失的 MuST-c v2 数据上进行训练。我们使用简单的固定保持策略来调整离线 ST 模型以适应 SST。实验表明，我们的模型在 MuST-C-v2 tst-COMMON 上获得了 31.1 的离线 BLEU 分数和 2 秒延迟下的 29.5 的 BLEU 分数。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07452
- **Authors**: Xi Xu,Siqi Ouyang,Brian Yan,Patrick Fernandes,William Chen,Lei Li,Graham Neubig,Shinji Watanabe
- **Abstract**: This paper describes CMU's submission to the IWSLT 2024 Simultaneous Speech Translation (SST) task for translating English speech to German text in a streaming manner. Our end-to-end speech-to-text (ST) system integrates the WavLM speech encoder, a modality adapter, and the Llama2-7B-Basemodelas the decoder. We employ a two-stage training approach: initially, we align the representations of speech and text, followed by full fine-tuning. Both stages are trained on MuST-c v2 data with cross-entropy loss. We adapt our offline STmodelfor SST using a simple fixed hold-n policy. Experiments show that ourmodelobtains an offline BLEU score of 31.1 and a BLEU score of 29.5 under 2 seconds latency on the MuST-C-v2 tst-COMMON.

### Fact or Fiction? Improving Fact Verification with Knowledge Graphs through Simplified Subgraph Retrievals
- **标题**: 事实还是虚构？通过简化的子图检索改进知识图的事实验证
- **Categories**: cs.CL cs.AI cs.LG
- **摘要**: 尽管自然语言处理（NLP）最近取得了成功，但事实验证仍然是一项艰巨的任务。由于错误信息传播得越来越快，人们的注意力已经转向自动验证声明的正确性。在 NLP 领域，这通常是通过训练有监督的机器学习模型来利用来自可信语料库的证据来验证主张来完成的。我们提出了验证数据集声明的有效方法，其中证据采用结构化知识图的形式。我们使用 FactKG 数据集，该数据集是根据从 Wikipedia 中提取的 DBpedia 知识图构建的。通过简化证据检索过程，从微调语言模型到简单的逻辑检索，我们能够构建需要更少计算资源并实现更好的测试集准确性的模型。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07453
- **Authors**: Tobias A. Opsahl
- **Abstract**: Despite recent success in naturallanguageprocessing (NLP), fact verification still remains a difficult task. Due to misinformation spreading increasingly fast, attention has been directed towards automatically verifying the correctness of claims. In the domain of NLP, this is usually done by training supervised machine learningmodelsto verify claims by utilizing evidence from trustworthy corpora. We present efficient methods for verifying claims on a dataset where the evidence is in the form of structured knowledge graphs. We use the FactKG dataset, which is constructed from the DBpedia knowledge graph extracted from Wikipedia. By simplifying the evidence retrieval process, from fine-tunedlanguagemodelsto simple logical retrievals, we are able to constructmodelsthat both require less computational resources and achieve better test-set accuracy.

### From Brazilian Portuguese to European Portuguese
- **标题**: 从巴西葡萄牙语到欧洲葡萄牙语
- **Categories**: cs.CL
- **摘要**: 巴西葡萄牙语和欧洲葡萄牙语是同一语言的两个变体，尽管它们非常相似，但它们也表现出一些差异。然而，这两种变体之间的资源可用性存在显着的不成比例，巴西葡萄牙语的资源更为丰富。这种不平等可能会影响欧洲葡萄牙语使用者获得的翻译服务的质量。为了解决这个问题，我们建议利用神经架构和模型的最新进展，开发巴西葡萄牙语到欧洲葡萄牙语的翻译系统。为了评估此类系统的性能，我们手动策划了一个黄金测试集，其中包含五个不同主题的 500 个句子。黄金测试集中的每个句子都有两个不同的参考，有助于对未来翻译模型进行直接评估。我们使用从巴西和欧洲葡萄牙语的电影字幕和 TED 演讲记录中提取的并行数据对现有的大型语言模型进行微调，从而对各种模型进行了实验。我们的评估涉及使用传统的自动指标以及人工评估。此外，所有模型都与 ChatGPT 3.5 Turbo 进行了比较，后者目前产生了最佳结果。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07457
- **Authors**: João Sanches,Rui Ribeiro,Luísa Coheur
- **Abstract**: Brazilian Portuguese and European Portuguese are two varieties of the samelanguageand, despite their close similarities, they exhibit several differences. However, there is a significant disproportion in the availability of resources between the two variants, with Brazilian Portuguese having more abundant resources. This inequity can impact the quality of translation services accessible to European Portuguese speakers. To address this issue, we propose the development of a Brazilian Portuguese to European Portuguese translation system, leveraging recent advancements in neural architectures andmodels. To evaluate the performance of such systems, we manually curated a gold test set comprising 500 sentences across five different topics. Each sentence in the gold test set has two distinct references, facilitating a straightforward evaluation of future translationmodels. We experimented with variousmodelsby fine-tuning existing LargeLanguageModelsusing parallel data extracted from movie subtitles and TED Talks transcripts in both Brazilian and European Portuguese. Our evaluation involved the use of conventional automatic metrics as well as a human evaluation. In addition, allmodelswere compared against ChatGPT 3.5 Turbo, which currently yields the best results.

### LargeLanguageModelsPrompting With Episodic Memory
- **标题**: 大型语言模型通过情景记忆进行提示
- **Categories**: cs.CL cs.AI
- **摘要**: 提示优化对于增强大型语言模型 (LLM) 在一系列自然语言处理 (NLP) 任务中的性能至关重要，特别是在训练示例直接合并到提示中的小样本学习场景中。尽管人们对通过少量示例优化提示越来越感兴趣，但现有的提示优化方法通常是资源密集型的或性能不足。在这项工作中，我们提出了POEM（Prompting with Episodic Memory，POEM），这是一种简单、高效且具有强大泛化能力的新型提示优化技术。我们将即时优化作为强化学习（RL）挑战，使用情景记忆来存档输入数据的组合、少数样本的排列以及训练期间观察到的奖励。在测试阶段，我们通过从情景记忆中前 k 个最相似的训练示例中选择产生最高总奖励的序列来优化每个测试查询的示例序列。我们的结果表明，在各种文本分类任务中，POEM 的性能比 TEMPERA 和 RLPrompt 等最新技术高出 5.3% 以上。此外，我们的方法很好地适应了更广泛的语言理解任务，始终优于排序示例的传统启发式方法。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07465
- **Authors**: Dai Do,Quan Tran,Svetha Venkatesh,Hung Le
- **Abstract**: Prompt optimization is essential for enhancing the performance of LargeLanguageModels(LLMs) in a range of NaturalLanguageProcessing (NLP) tasks, particularly in scenarios of few-shot learning where training examples are incorporated directly into the prompt. Despite the growing interest in optimizing prompts with few-shot examples, existing methods for prompt optimization are often resource-intensive or perform inadequately. In this work, we propose PrOmpting with Episodic Memory (POEM), a novel prompt optimization technique that is simple, efficient, and demonstrates strong generalization capabilities. We approach prompt optimization as a Reinforcement Learning (RL) challenge, using episodic memory to archive combinations of input data, permutations of few-shot examples, and the rewards observed during training. In the testing phase, we optimize the sequence of examples for each test query by selecting the sequence that yields the highest total rewards from the top-k most similar training examples in the episodic memory. Our results show that POEM outperforms recent techniques like TEMPERA and RLPrompt by over 5.3% in various text classification tasks. Furthermore, our approach adapts well to broaderlanguageunderstanding tasks, consistently outperforming conventional heuristic methods for ordering examples.

### Bridging andModelingCorrelations in Pairwise Data for Direct Preference Optimization
- **标题**: 用于直接偏好优化的成对数据中的桥接和建模相关性
- **Categories**: cs.CL
- **摘要**: 直接偏好优化（DPO）是一种广泛采用的离线偏好优化算法，旨在使用成对偏好数据将大型语言模型（LLM）与人类期望的行为结合起来。然而，成对数据中的获胜响应和失败响应是单独生成的，导致它们之间的相关性较弱以及对齐性能不佳。为了解决这个问题，我们提出了一个名为 BMC 的有效框架，用于桥接和建模成对数据中的相关性。首先，我们通过有针对性的修改来提高成对偏好信号的一致性和信息量，通过基于获胜响应改进失败响应来合成伪获胜响应。其次，我们发现仅 DPO 不足以模拟这些相关性并捕获细微的变化。因此，我们建议通过在训练期间动态利用策略模型的置信度来学习令牌级别的相关性。关于 QA、数学和指令遵循任务的综合实验证明了我们方法的有效性，显着超越了包括 DPO 在内的竞争基准。此外，我们深入的定量分析揭示了我们的方法优于 DPO 的原因，并展示了其与其他 DPO 变体的多功能性。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07471
- **Authors**: Yuxin Jiang,Bo Huang,Yufei Wang,Xingshan Zeng,Liangyou Li,Yasheng Wang,Xin Jiang,Lifeng Shang,Ruiming Tang,Wei Wang
- **Abstract**: Direct preference optimization (DPO), a widely adopted offline preference optimization algorithm, aims to align largelanguagemodels(LLMs) with human-desired behaviors using pairwise preference data. However, the winning response and the losing response within pairwise data are generated isolatedly, leading to weak correlations between them as well as suboptimal alignment performance. To address this issue, we propose an effective framework named BMC, for bridging andmodelingcorrelations in pairwise data. Firstly, we increase the consistency and informativeness of the pairwise preference signals by targeted modifications, synthesizing a pseudo winning response through improving the losing response based on the winning response. Secondly, we identify that DPO alone is insufficient tomodelthese correlations and capture nuanced variations. Therefore, we propose learning token-level correlations by dynamically leveraging the policymodel'sconfidence during training. Comprehensive experiments on QA, math, and instruction-following tasks demonstrate the effectiveness of our approach, significantly surpassing competitive baselines, including DPO. Additionally, our in-depth quantitative analysis reveals the reasons behind our method's superior performance over DPO and showcases its versatility to other DPO variants.

### Unsupervised Blind Joint Dereverberation and Room Acoustics Estimation with DiffusionModels
- **标题**: 使用扩散模型进行无监督盲联合混响和室内声学估计
- **Categories**: eess.AS cs.LG cs.SD
- **摘要**: 本文提出了一种用于单通道盲去混响和房间脉冲响应 (RIR) 估计的无监督方法，称为 BUDDy。该算法植根于贝叶斯后验采样：它结合了增强混响测量保真度的似然模型和由无条件扩散模型实现的消声语音先验。我们设计了一个代表 RIR 的参数滤波器，每个频率子带都有指数衰减。室内声学估计和语音去混响是联合进行的，因为滤波器参数被迭代估计并且语音话语沿着反向扩散轨迹被细化。在房间脉冲响应未知的盲场景中，BUDDy 在各种声学场景中成功执行语音去混响，显着优于其他盲无监督基线。与通常难以概括的监督方法不同，BUDDy 可以无缝地适应不同的声学条件。本文通过提供新的实验结果以及对算法性能和多功能性的见解来扩展我们之前的工作。我们首先研究知情去混响方法对 RIR 估计误差的鲁棒性，以激发联合声学估计和去混响范式。然后，我们展示了我们的方法对高分辨率歌声去混响的适应性，研究其在 RIR 估计中的性能，并进行主观评估实验以验证结果的感知质量等。音频样本和代码可以在线找到。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07472
- **Authors**: Jean-Marie Lemercier,Eloi Moliner,Simon Welker,Vesa Välimäki,Timo Gerkmann
- **Abstract**: This paper presents an unsupervised method for single-channel blind dereverberation and room impulse response (RIR) estimation, called BUDDy. The algorithm is rooted in Bayesian posterior sampling: it combines a likelihoodmodelenforcing fidelity to the reverberant measurement, and an anechoic speech prior implemented by an unconditional diffusionmodel. We design a parametric filter representing the RIR, with exponential decay for each frequency subband. Room acoustics estimation and speech dereverberation are jointly carried out, as the filter parameters are iteratively estimated and the speech utterance refined along the reverse diffusion trajectory. In a blind scenario where the room impulse response is unknown, BUDDy successfully performs speech dereverberation in various acoustic scenarios, significantly outperforming other blind unsupervised baselines. Unlike supervised methods, which often struggle to generalize, BUDDy seamlessly adapts to different acoustic conditions. This paper extends our previous work by offering new experimental results and insights into the algorithm's performance and versatility. We first investigate the robustness of informed dereverberation methods to RIR estimation errors, to motivate the joint acoustic estimation and dereverberation paradigm. Then, we demonstrate the adaptability of our method to high-resolution singing voice dereverberation, study its performance in RIR estimation, and conduct subjective evaluation experiments to validate the perceptual quality of the results, among other contributions. Audio samples and code can be found online.

### A Study on Bias Detection and Classification in NaturalLanguageProcessing
- **标题**: 自然语言处理中的偏差检测和分类研究
- **Categories**: cs.CL cs.AI
- **摘要**: 人类偏见已被证明会影响各个领域的模型和算法的性能，包括自然语言处理。尽管近年来对这一现象的研究引起了人们的关注，但可用资源仍然相对稀缺，通常集中在不同形式或表现形式的偏见上。我们工作的目标有两个：1）收集公开可用的数据集，并确定如何更好地组合它们，以有效地训练仇恨语音检测和分类任务中的模型； 2）分析这些数据集的主要问题，例如稀缺性、资源倾斜以及对非持久数据的依赖。我们与实验的发展一起讨论了这些问题，其中我们表明不同数据集的组合极大地影响了模型的性能。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07479
- **Authors**: Ana Sofia Evans,Helena Moniz,Luísa Coheur
- **Abstract**: Human biases have been shown to influence the performance ofmodelsand algorithms in various fields, including NaturalLanguageProcessing. While the study of this phenomenon is garnering focus in recent years, the available resources are still relatively scarce, often focusing on different forms or manifestations of biases. The aim of our work is twofold: 1) gather publicly-available datasets and determine how to better combine them to effectively trainmodelsin the task of hate speech detection and classification; 2) analyse the main issues with these datasets, such as scarcity, skewed resources, and reliance on non-persistent data. We discuss these issues in tandem with the development of our experiments, in which we show that the combinations of different datasets greatly impact themodels' performance.

### Training Overhead Ratio: A Practical Reliability Metric for LargeLanguageModelTraining Systems
- **标题**: 训练开销比：LargeLanguageModelTraining 系统的实用可靠性指标
- **Categories**: cs.DC cs.AI
- **摘要**: LargeLanguageModels（LLM）以其卓越的能力正在彻底改变人工智能行业。训练这些模型需要大规模的 GPU 集群和大量的计算时间，导致频繁失败，从而显着增加训练成本。尽管它很重要，但该领域缺乏评估可靠性的指标。在这项工作中，我们引入了一种名为 \emph{Training Overhead Ratio} (TOR) 的新型可靠性指标来评估容错 LLM 训练系统的可靠性。 TOR 被定义为系统的最佳训练时间与观察到的训练时间的比率，作为用户估计训练给定系统的 LLMon 所需的实际时间的实用工具。此外，我们的调查确定了提高可靠性的关键因素，并针对实践中遇到的各种类型的故障提出了 TOR 方程。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07482
- **Authors**: Ning Lu,Qian Xie,Hao Zhang,Wenyi Fang,Yang Zheng,Jiantao Ma
- **Abstract**: LargeLanguageModels(LLMs) are revolutionizing the AI industry with their superior capabilities. Training thesemodelsrequires large-scale GPU clusters and significant computing time, leading to frequent failures that significantly increase training costs. Despite its significance, this field lacks a metric for evaluating reliability. In this work, we introduce a novel reliability metric called \emph{Training Overhead Ratio} (TOR) to evaluate the reliability of fault-tolerantLLMtraining systems. TOR is defined as the ratio of optimal training time to the observed training time of a system, serving as a practical tool for users to estimate the actual time required to train anLLMon a given system. Furthermore, our investigation identifies the key factor for enhancing reliability and present TOR equations for various types of failures encountered in practice.

### Towards Enhanced Context Awareness with Vision-basedMultimodalInterfaces
- **标题**: 通过基于视觉的多模态接口增强情境感知
- **Categories**: cs.HC doi 10.1145/3640471.3686646
- **摘要**: 基于视觉的界面 (VI) 对于推进人机交互 (HCI) 至关重要，特别是在增强情境感知方面。然而，由于多模式人工智能 (AI) 的快速发展，这些接口存在重大机遇，这预示着人类与智能系统之间紧密耦合的未来。人工智能驱动的 VI 与其他模式集成时，可以提供强大的解决方案，有效捕获和解释用户意图和复杂的环境信息，从而促进无缝、高效的交互。本博士研究探索了多模态界面增强情境感知的三个应用案例，分别关注视觉模态的三个维度：尺度、深度和时间：通过显微图像对物理表面进行细粒度分析，利用深度数据对现实世界进行精确投影，并在虚拟环境中渲染来自视频背景的触觉反馈。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07488
- **Authors**: Yongquan Hu,Wen Hu,Aaron Quigley
- **Abstract**: Vision-based Interfaces (VIs) are pivotal in advancing Human-Computer Interaction (HCI), particularly in enhancing context awareness. However, there are significant opportunities for these interfaces due to rapid advancements inmultimodalArtificial Intelligence (AI), which promise a future of tight coupling between humans and intelligent systems. AI-driven VIs, when integrated with other modalities, offer a robust solution for effectively capturing and interpreting user intentions and complex environmental information, thereby facilitating seamless and efficient interactions. This PhD study explores three application cases ofmultimodalinterfaces to augment context awareness, respectively focusing on three dimensions of visual modality: scale, depth, and time: a fine-grained analysis of physical surfaces via microscopic image, precise projection of the real world using depth data, and rendering haptic feedback from video background in virtual environments.

### QirK: Question Answering via Intermediate Representation on Knowledge Graphs
- **标题**: QirK：通过知识图上的中间表示进行问答
- **Categories**: cs.DB cs.LG
- **摘要**: 我们演示了 QirK，一个用于回答知识图（KG）上的自然语言问题的系统。 QirK 可以回答结构复杂的问题，这些问题仍然超出新兴的大型语言模型 (LLM) 的能力范围。它使用数据库技术、法学硕士和向量嵌入语义搜索的独特组合来实现这一点。这些组件的粘合剂是中间表示（IR）。使用LLM将输入问题映射到IR，然后借助向量嵌入的语义搜索将其修复为有效的关系数据库查询。这使得 LLM 能力和 KG 可靠性的实际综合成为可能。
  演示 QirK 的短视频可在 https://youtu.be/6c81BLmOZ0U 上观看。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07494
- **Authors**: Jan Luca Scheerer,Anton Lykov,Moe Kayali,Ilias Fountalis,Dan Olteanu,Nikolaos Vasiloglou,Dan Suciu
- **Abstract**: We demonstrate QirK, a system for answering naturallanguagequestions on Knowledge Graphs (KG). QirK can answer structurally complex questions that are still beyond the reach of emerging LargeLanguageModels(LLMs). It does so using a unique combination of database technology,LLMs, and semantic search over vector embeddings. The glue for these components is an intermediate representation (IR). The input question is mapped to IR usingLLMs, which is then repaired into a valid relational database query with the aid of a semantic search on vector embeddings. This allows a practical synthesis ofLLMcapabilities and KG reliability.
  A short video demonstrating QirK is available at https://youtu.be/6c81BLmOZ0U.

### Cross-Platform Video Person ReID: A New Benchmark Dataset and Adaptation Approach
- **标题**: 跨平台视频行人重识别：新的基准数据集和适应方法
- **Categories**: cs.CV
- **摘要**: 在本文中，我们构建了一个基于地对空视频的行人重新识别的大规模基准数据集，名为 G2A-VReID，其中包含 185,907 张图像和 5,576 个轨迹，具有 2,788 个不同的身份。据我们所知，这是地对空场景下的第一个视频 ReID 数据集。 G2A-VReID数据集具有以下特点：1）视图变化剧烈； 2）大量标注身份； 3）丰富的户外场景； 4）分辨率差异巨大。此外，我们提出了一种新的跨平台 ReID 基准方法，通过视觉语言模型（即 CLIP）将跨平台视觉对齐问题转化为视觉语义对齐，并应用参数高效的视频集级适配器模块将基于图像的基础模型适应视频 ReID 任务，称为 VSLA-CLIP。此外，为了进一步减少平台之间的巨大差异，我们还设计了平台桥提示，以实现高效的视觉特征对齐。大量实验证明了该方法在所有现有视频 ReID 数据集和我们提出的 G2A-VReID 数据集上的优越性。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07500
- **Authors**: Shizhou Zhang,Wenlong Luo,De Cheng,Qingchun Yang,Lingyan Ran,Yinghui Xing,Yanning Zhang
- **Abstract**: In this paper, we construct a large-scale benchmark dataset for Ground-to-Aerial Video-based person Re-Identification, named G2A-VReID, which comprises 185,907 images and 5,576 tracklets, featuring 2,788 distinct identities. To our knowledge, this is the first dataset for video ReID under Ground-to-Aerial scenarios. G2A-VReID dataset has the following characteristics: 1) Drastic view changes; 2) Large number of annotated identities; 3) Rich outdoor scenarios; 4) Huge difference in resolution. Additionally, we propose a new benchmark approach for cross-platform ReID by transforming the cross-platform visual alignment problem into visual-semantic alignment through vision-languagemodel(i.e., CLIP) and applying a parameter-efficient Video Set-Level-Adapter module to adapt image-based foundationmodelto video ReID tasks, termed VSLA-CLIP. Besides, to further reduce the great discrepancy across the platforms, we also devise the platform-bridge prompts for efficient visual feature alignment. Extensive experiments demonstrate the superiority of the proposed method on all existing video ReID datasets and our proposed G2A-VReID dataset.

### LargeLanguageModelsKnow What Makes Exemplary Contexts
- **标题**: 大型语言模型知道什么构成了示例性上下文
- **Categories**: cs.CL
- **摘要**: 随着大型语言模型（LLM）的进步，情境学习（ICL）已被证明是一种重要的能力。通过指导法学硕士使用少量演示示例，ICL 使他们能够执行广泛的任务，而无需更新数百万个参数。本文为法学硕士提出了一个统一的框架，允许他们自行选择有影响力的上下文示例来构建他们的上下文；对具有不同示范作文的候选人进行自我排名；通过强化学习自我优化演示选择和排序。具体来说，我们的方法设计了一个参数高效的检索头，该检索头在训练后生成优化的演示，并根据 LLM 自己的偏好进行奖励。实验结果验证了该方法在增强 ICL 性能方面的有效性。此外，我们的方法有效地识别和选择当前任务最具代表性的示例，并包括更多的检索多样性。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07505
- **Authors**: Quanyu Long,Jianda Chen
- **Abstract**: In-context learning (ICL) has proven to be a significant capability with the advancement of LargeLanguagemodels(LLMs). By instructingLLMsusing few-shot demonstrative examples, ICL enables them to perform a wide range of tasks without needing to update millions of parameters. This paper presents a unified framework forLLMsthat allows them to self-select influential in-context examples to compose their contexts; self-rank candidates with different demonstration compositions; self-optimize the demonstration selection and ordering through reinforcement learning. Specifically, our method designs a parameter-efficient retrieval head that generates the optimized demonstration after training with rewards fromLLM'sown preference. Experimental results validate the proposed method's effectiveness in enhancing ICL performance. Additionally, our approach effectively identifies and selects the most representative examples for the current task, and includes more diversity in retrieval.

### Dinkel: Testing Graph Database Engines via State-Aware Query Generation
- **标题**: Dinkel：通过状态感知查询生成测试图形数据库引擎
- **Categories**: cs.DB cs.SE
- **摘要**: 图数据库管理系统（GDBMS）存储和操作图数据，并构成许多数据驱动应用程序的核心部分。为了确保其可靠性，人们提出了几种通过使用最流行的图形查询语言 Cypher 生成查询来测试 GDBMS 的方法。然而，Cypher 允许查询具有复杂的状态变化和数据依赖关系，而现有方法不支持这些查询，因此无法生成有效的复杂查询，从而遗漏了 GDBMS 中的许多错误。
  在本文中，我们提出了一种新颖的状态感知测试方法来为 GDBMS 生成复杂的 Cypher 查询。我们的方法对两种图状态、查询上下文和图模式进行建模。查询上下文描述了可用的 Cypher 变量及其相应的范围，而图模式则总结了所操作的图标签和属性。在生成 Cypher 查询时，我们动态修改图状态，以确保查询中的每个子句都可以引用正确的状态信息。通过这种方式，我们的方法可以生成具有多个状态变化和复杂数据依赖关系的 Cypher 查询，同时保持较高的查询有效性。我们将此方法实现为全自动 GDBMS 测试框架 Dinkel，并在三个流行的开源 GDBMS（即 Neo4j、RedisGraph 和 Apache AGE）上对其进行了评估。 Dinkel 总共发现了 60 个 bug，其中确认了 58 个，修复了 51 个。我们的评估结果表明，Dinkel 可以有效地生成复杂查询，且有效性较高（93.43%）。与现有方法相比，Dinkel 在 48 小时的测试活动中可以覆盖超过 60% 的代码并发现更多的错误。我们期望 Dinkel 强大的测试用例生成功能能够使 GDBMS 测试受益，并有助于增强 GDBMS 的可靠性。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07525
- **Authors**: Dominic Wüst,Zu-Ming Jiang,Zhendong Su
- **Abstract**: Graph database management systems (GDBMSs) store and manipulate graph data and form a core part of many data-driven applications. To ensure their reliability, several approaches have been proposed to test GDBMSs by generating queries in Cypher, the most popular graph querylanguage. However, Cypher allows queries with complicated state changes and data dependencies, which existing approaches do not support and thus fail to generate valid, complex queries, thereby missing many bugs in GDBMSs.
  In this paper, we propose a novel state-aware testing approach to generate complex Cypher queries for GDBMSs. Our approachmodelstwo kinds of graph state, query context and graph schema. Query context describes the available Cypher variables and their corresponding scopes, whereas graph schema summarizes the manipulated graph labels and properties. While generating Cypher queries, we modify the graph states on the fly to ensure each clause within the query can reference the correct state information. In this way, our approach can generate Cypher queries with multiple state changes and complicated data dependencies while retaining high query validity. We implemented this approach as a fully automatic GDBMS testing framework, Dinkel, and evaluated it on three popular open-source GDBMSs, namely Neo4j, RedisGraph, and Apache AGE. In total, Dinkel found 60 bugs, among which 58 were confirmed and 51 fixed. Our evaluation results show that Dinkel can effectively generate complex queries with high validity (93.43%). Compared to existing approaches, Dinkel can cover over 60% more code and find more bugs within the 48-hour testing campaign. We expect Dinkel's powerful test-case generation to benefit GDBMS testing and help strengthen the reliability of GDBMSs.

### Learning-basedModelsfor Vulnerability Detection: An Extensive Study
- **标题**: 基于学习的漏洞检测模型：广泛研究
- **Categories**: cs.SE cs.CR cs.LG
- **摘要**: 尽管许多基于深度学习的模型在漏洞检测方面取得了长足的进步，但我们对这些模型的了解还不够深入，这限制了模型能力的进一步提升、对模型检测机制的理解以及模型实际应用的效率和安全性。在本文中，我们通过在最近构建的大规模数据集上进行实验，广泛而全面地研究了两种最先进的基于学习的方法（基于序列和基于图）。我们从模型能力、模型解释、模型稳定性、模型易用性和模型经济性五个维度调查了七个研究问题。我们通过实验证明了基于序列的模型的优先级以及LLM（ChatGPT）和基于图的模型的有限能力。我们探索了基于学习的模型所擅长的漏洞类型，并揭示了模型的不稳定性，尽管输入在语义上发生了微妙的等效变化。我们凭经验解释模型学到了什么。我们总结了预处理以及轻松使用模型的要求。最后，我们初步归纳了经济、安全地实际使用这些模型的重要信息。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07526
- **Authors**: Chao Ni,Liyu Shen,Xiaodan Xu,Xin Yin,Shaohua Wang
- **Abstract**: Though many deep learning-basedmodelshave made great progress in vulnerability detection, we have no good understanding of thesemodels, which limits the further advancement ofmodelcapability, understanding of the mechanism ofmodeldetection, and efficiency and safety of practical application ofmodels. In this paper, we extensively and comprehensively investigate two types of state-of-the-art learning-based approaches (sequence-based and graph-based) by conducting experiments on a recently built large-scale dataset. We investigate seven research questions from five dimensions, namelymodelcapabilities,modelinterpretation,modelstability, ease of use ofmodel, andmodeleconomy. We experimentally demonstrate the priority of sequence-basedmodelsand the limited abilities of bothLLM(ChatGPT) and graph-basedmodels. We explore the types of vulnerability that learning-basedmodelsskilled in and reveal the instability of themodelsthough the input is subtlely semantical-equivalently changed. We empirically explain what themodelshave learned. We summarize the pre-processing as well as requirements for easily using themodels. Finally, we initially induce the vital information for economically and safely practical usage of thesemodels.

### Development of a Multi-Agent Clinical Decision Support System for Korean Triage and Acuity Scale (KTAS)-Based Triage and Treatment Planning in Emergency Departments
- **标题**: 为急诊科基于韩国分诊和敏锐度量表 (KTAS) 的分诊和治疗计划开发多智能体临床决策支持系统
- **Categories**: cs.AI cs.CL cs.LG
- **摘要**: 急诊科 (ED) 过度拥挤以及重症监护环境中快速决策的复杂性给全球医疗保健系统带来了重大挑战。虽然临床决策支持系统 (CDSS) 已显示出希望，但大语言模型 (LLM) 的集成为提高分诊准确性和临床决策提供了新的可能性。本研究提出了一个由法学硕士驱动的 CDSS，旨在协助急诊医生和护士进行患者分类、治疗计划和整体急诊护理管理。
  我们利用 Llama-3-70b 作为基础 LLM 开发了一个多代理 CDSS，由 CrewAI 和 Langchain 精心策划。该系统由四个模拟急诊室关键角色的人工智能代理组成：分诊护士、急诊医生、药剂师和急诊室协调员。它采用韩国分诊和敏锐度量表 (KTAS) 进行分诊评估，并与 RxNorm API 集成进行药物管理。
  该模型使用 Asclepius 数据集进行评估，并由临床急诊医学专家评估性能。与单代理系统的基线相比，CDSS 在分类决策方面表现出较高的准确性。此外，该系统在关键领域表现出强大的性能，包括初步诊断、关键发现识别、处置决策、治疗计划和资源分配。
  我们的多代理 CDSS 展示了支持综合紧急护理管理的巨大潜力。通过利用最先进的人工智能技术，该系统提供了一种可扩展且适应性强的工具，可以增强紧急医疗服务的提供，有可能缓解急诊室的过度拥挤并改善患者的治疗结果。这项工作有助于人工智能在急诊医学领域不断发展的应用，并为未来的研究和临床实施提供了有前景的方向。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07531
- **Authors**: Seungjun Han,Wongyung Choi
- **Abstract**: Emergency department (ED) overcrowding and the complexity of rapid decision-making in critical care settings pose significant challenges to healthcare systems worldwide. While clinical decision support systems (CDSS) have shown promise, the integration of largelanguagemodels(LLMs) offers new possibilities for enhancing triage accuracy and clinical decision-making. This study presents anLLM-driven CDSS designed to assist ED physicians and nurses in patient triage, treatment planning, and overall emergency care management.
  We developed a multi-agent CDSS utilizing Llama-3-70b as the baseLLM, orchestrated by CrewAI and Langchain. The system comprises four AI agents emulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED Coordinator. It incorporates the Korean Triage and Acuity Scale (KTAS) for triage assessment and integrates with the RxNorm API for medication management.
  Themodelwas evaluated using the Asclepius dataset, with performance assessed by a clinical emergency medicine specialist. The CDSS demonstrated high accuracy in triage decision-making compared to the baseline of a single-agent system. Furthermore, the system exhibited strong performance in critical areas, including primary diagnosis, critical findings identification, disposition decision-making, treatment planning, and resource allocation.
  Our multi-agent CDSS demonstrates significant potential for supporting comprehensive emergency care management. By leveraging state-of-the-art AI technologies, this system offers a scalable and adaptable tool that could enhance emergency medical care delivery, potentially alleviating ED overcrowding and improving patient outcomes. This work contributes to the growing field of AI applications in emergency medicine and offers a promising direction for future research and clinical implementation.

### Usefulness of data flow diagrams and largelanguagemodelsfor security threat validation: a registered report
- **标题**: 数据流图和大型语言模型在安全威胁验证中的实用性：注册报告
- **Categories**: cs.SE
- **摘要**: 最近网络安全标准的到来提高了组织安全评估的标准，但现有技术并不总是能够很好地扩展。威胁分析和风险评估用于识别新系统或重构系统的安全威胁。尽管如此，仍然缺乏完成的定义，因此必须验证已识别的威胁，这会减慢分析速度。现有文献主要关注威胁分析的整体性能，但之前没有研究过分析人员必须深入研究材料才能有效验证已识别的安全威胁。我们建议与从业者进行一项对照实验，以调查某些分析材料（例如法学硕士生成的建议）是否比没有更好，以及更多材料（系统的数据流图和法学硕士生成的建议）是否比某些材料更好。此外，我们还介绍了对 41 名理学硕士学生进行试点的主要结果，这些结果可用于改进研究设计。最后，我们还提供了一个初始复制包，包括实验材料和数据分析脚本，以及一项扩展计划，以包括基于与从业者的最终数据收集活动（例如预筛选问题）的新材料。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07537
- **Authors**: Winnie Bahati Mbaka,Katja Tuma
- **Abstract**: The arrival of recent cybersecurity standards has raised the bar for security assessments in organizations, but existing techniques don't always scale well. Threat analysis and risk assessment are used to identify security threats for new or refactored systems. Still, there is a lack of definition-of-done, so identified threats have to be validated which slows down the analysis. Existing literature has focused on the overall performance of threat analysis, but no previous work has investigated how deep must the analysts dig into the material before they can effectively validate the identified security threats. We propose a controlled experiment with practitioners to investigate whether some analysis material (likeLLM-generated advice) is better than none and whether more material (the system's data flow diagram andLLM-generated advice) is better than some material. In addition, we present key findings from running a pilot with 41 MSc students, which are used to improve the study design. Finally, we also provide an initial replication package, including experimental material and data analysis scripts and a plan to extend it to include new materials based on the final data collection campaign with practitioners (e.g., pre-screening questions).

### Cross-aware Early Fusion with Stage-divided Vision andLanguageTransformer Encoders for Referring Image Segmentation
- **标题**: 跨阶段感知早期融合与用于参考图像分割的分阶段视觉和语言转换器编码器
- **Categories**: cs.CV cs.AI doi 10.1109/TMM.2023.3340062
- **摘要**: 引用分割旨在分割与自然语言表达相关的目标对象。该任务的主要挑战是理解复杂且模糊的语言表达的含义，并通过参考表达来确定具有多个对象的图像中的相关区域。最近的模型侧重于视觉编码器中间阶段与语言特征的早期融合，但这些方法有一个局限性，即语言特征无法引用视觉信息。为了解决这个问题，本文提出了一种新颖的架构，即具有阶段划分的视觉和语言转换器编码器的交叉感知早期融合（CrossVLT），它允许语言和视觉编码器进行早期融合，以提高跨模态上下文建模的能力。与以前的方法不同，我们的方法使视觉和语言特征能够在每个阶段引用彼此的信息，以相互增强两个编码器的鲁棒性。此外，与仅依赖于跨模态对齐的高级特征的传统方案不同，我们引入了一种基于特征的对齐方案，该方案使得视觉和语言编码器的低级到高级特征能够参与交叉模态对齐。 -模态对齐。通过对齐所有编码器阶段的中间跨模态特征，该方案实现了有效的跨模态融合。通过这种方式，所提出的方法对于参考图像分割来说是简单而有效的，并且在三个公共基准上优于之前最先进的方法。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07539
- **Authors**: Yubin Cho,Hyunwoo Yu,Suk-ju Kang
- **Abstract**: Referring segmentation aims to segment a target object related to a naturallanguageexpression. Key challenges of this task are understanding the meaning of complex and ambiguouslanguageexpressions and determining the relevant regions in the image with multiple objects by referring to the expression. Recentmodelshave focused on the early fusion with thelanguagefeatures at the intermediate stage of the vision encoder, but these approaches have a limitation that thelanguagefeatures cannot refer to the visual information. To address this issue, this paper proposes a novel architecture, Cross-aware early fusion with stage-divided Vision andLanguageTransformer encoders (CrossVLT), which allows bothlanguageand vision encoders to perform the early fusion for improving the ability of the cross-modal contextmodeling. Unlike previous methods, our method enables the vision andlanguagefeatures to refer to each other's information at each stage to mutually enhance the robustness of both encoders. Furthermore, unlike the conventional scheme that relies solely on the high-level features for the cross-modal alignment, we introduce a feature-based alignment scheme that enables the low-level to high-level features of the vision andlanguageencoders to engage in the cross-modal alignment. By aligning the intermediate cross-modal features in all encoder stages, this scheme leads to effective cross-modal fusion. In this way, the proposed approach is simple but effective for referring image segmentation, and it outperforms the previous state-of-the-art methods on three public benchmarks.

### New Curriculum, New Chance -- Retrieval Augmented Generation for Lesson Planning in Ugandan Secondary Schools. Prototype Quality Evaluation
- **标题**: 新课程，新机会——乌干达中学备课的检索增强生成。原型质量评估
- **Categories**: cs.CY cs.AI cs.IR cs.LG
- **摘要**: 简介： 中学教育质量差仍然被视为 21 世纪乌干达的主要难题之一，尤其是在农村地区。研究发现了几个问题，包括质量低下或教师缺席的课程计划。随着政府推动新课程的实施，现有的课程计划变得过时，问题变得更加严重。使用检索增强生成方法，我们开发了一个原型，可以根据政府认可的教科书生成定制的课程计划。这有助于教师更有效、更高质量地制定课程计划，确保他们完全符合新课程和基于能力的学习方法。
  方法：原型是使用 CohereLLMand 句子嵌入和 LangChain 框架创建的 - 然后在公共网站上提供。矢量商店接受了三本新课程教科书（信息通信技术、数学、历史）的培训，全部为中学一年级课程。根据教科书中建议的时间段，按照伪随机生成协议生成了二十四个课程计划。三位独立评估者按照 Ndihokubwayo 等人的课程计划分析协议 (LPAP) 对课程计划的技术质量进行了分析。 （2022）是专门为东非和基于能力的课程设计的。
  结果：使用 LPAP 对 24 个课程计划进行评估，平均质量在 75% 到 80% 之间，相当于“非常好的课程计划”。尽管有一个课程计划可能被认为遗漏了该主题，但没有一个课程计划的得分低于 65%。总之，生成的教案的质量即使不是更好，也至少与人类创建的教案相当，卢旺达的一项研究表明，没有一个教案达到 50% 的基准。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07542
- **Authors**: Simon Kloker,Herbertson Bukoli,Twaha Kateete
- **Abstract**: Introduction: Poor educational quality in Secondary Schools is still regarded as one of the major struggles in 21st century Uganda - especially in rural areas. Research identifies several problems, including low quality or absent teacher lesson planning. As the government pushes towards the implementation of a new curriculum, exiting lesson plans become obsolete and the problem is worsened. Using a Retrieval Augmented Generation approach, we developed a prototype that generates customized lesson plans based on the government-accredited textbooks. This helps teachers create lesson plans more efficiently and with better quality, ensuring they are fully aligned the new curriculum and the competence-based learning approach.
  Methods: The prototype was created using CohereLLMand Sentence Embeddings, and LangChain Framework - and thereafter made available on a public website. Vector stores were trained for three new curriculum textbooks (ICT, Mathematics, History), all at Secondary 1 Level. Twenty-four lessons plans were generated following a pseudo-random generation protocol, based on the suggested periods in the textbooks. The lesson plans were analyzed regarding their technical quality by three independent raters following the Lesson Plan Analysis Protocol (LPAP) by Ndihokubwayo et al. (2022) that is specifically designed for East Africa and competence-based curriculums.
  Results: Evaluation of 24 lesson plans using the LPAP resulted in an average quality of between 75 and 80%, corresponding to "very good lesson plan". None of the lesson plans scored below 65%, although one lesson plan could be argued to have been missing the topic. In conclusion, the quality of the generated lesson plans is at least comparable, if not better, than those created by humans, as demonstrated in a study in Rwanda, whereby no lesson plan even reached the benchmark of 50%.

### MathScape: Evaluating MLLMs inmultimodalMath Scenarios through a Hierarchical Benchmark
- **标题**: MathScape：通过分层基准评估多模式数学场景中的 MLLM
- **Categories**: cs.CV cs.CL
- **摘要**: 随着多模态大语言模型（MLLM）的发展，数学问题背景下多模态模型的评估已成为一个有价值的研究领域。多模态视觉文本数学推理是评估MLLM的理解能力和复杂的多步定量推理能力的关键指标。然而，以前的多模态数学基准没有充分整合视觉和文本信息。为了解决这一差距，我们提出了 MathScape，这是一个新的基准，强调对视觉和文本信息组合的理解和应用。 MathScape 旨在评估基于照片的数学问题场景，通过分类分层方法评估 MLLM 的理论理解和应用能力。我们对 11 个先进的 MLLM 进行了多维评估，结果表明，即使对于最复杂的模型，我们的基准也具有挑战性。通过分析评估结果，我们确定了 MLLM 的局限性，为提高模型性能提供了宝贵的见解。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07543
- **Authors**: Minxuan Zhou,Hao Liang,Tianpeng Li,Zhiyu Wu,Mingan Lin,Linzhuang Sun,Yaqi Zhou,Yan Zhang,Xiaoqin Huang,Yicong Chen,Yujing Qiao,Weipeng Chen,Bin Cui,Wentao Zhang,Zenan Zhou
- **Abstract**: With the development ofMultimodalLargeLanguageModels(MLLMs), the evaluation ofmultimodalmodelsin the context of mathematical problems has become a valuable research field.Multimodalvisual-textual mathematical reasoning serves as a critical indicator for evaluating the comprehension and complex multi-step quantitative reasoning abilities of MLLMs. However, previousmultimodalmath benchmarks have not sufficiently integrated visual and textual information. To address this gap, we proposed MathScape, a new benchmark that emphasizes the understanding and application of combined visual and textual information. MathScape is designed to evaluate photo-based math problem scenarios, assessing the theoretical understanding and application ability of MLLMs through a categorical hierarchical approach. We conduct a multi-dimensional evaluation on 11 advanced MLLMs, revealing that our benchmark is challenging even for the most sophisticatedmodels. By analyzing the evaluation results, we identify the limitations of MLLMs, offering valuable insights for enhancingmodelperformance.

### Transformers and LargeLanguageModelsfor Efficient Intrusion Detection Systems: A Comprehensive Survey
- **标题**: 用于高效入侵检测系统的变压器和大型语言模型：综合调查
- **Categories**: cs.CR cs.AI cs.CL cs.CV eess.AS
- **摘要**: 随着 TransformersLLM 的显着进步，NLP 因其增强的文本生成和用户交互能力而将其影响范围扩展到许多研究领域。从这些进步中受益匪浅的一个领域是网络安全。在网络安全中，发送者和接收者之间需要保护和交换的许多参数都是文本和表格数据的形式，这使得 NLP 成为增强通信协议安全措施的宝贵工具。本调查论文对网络威胁检测系统中 Transformers 和 LLM 的使用进行了全面分析。概述了论文选择和文献计量分析的方法，以建立评估现有研究的严格框架。讨论了 Transformers 的基础知识，包括有关各种网络攻击的背景信息和该领域常用的数据集。该调查探讨了 Transformer 在 IDS 中的应用，重点关注不同的架构，例如基于注意力的模型、LLM（如 BERT 和 GPT）、CNN/LSTM-Transformer 混合体、新兴方法（如 ViT）等。此外，它还探讨了 Transformer 和基于 LLM 的 IDS 已实施的各种环境和应用，包括计算机网络、物联网设备、关键基础设施保护、云计算、SDN 以及自动驾驶汽车。本文还讨论了该领域的研究挑战和未来方向，确定了可解释性、可扩展性和对不断变化的威胁的适应性等关键问题。最后，结论总结了研究结果，强调了 Transformer 和法学硕士在增强网络威胁检测能力方面的重要性，同时概述了进一步研究和开发的潜在途径。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07583
- **Authors**: Hamza Kheddar
- **Abstract**: With significant advancements in TransformersLLMs, NLP has extended its reach into many research fields due to its enhanced capabilities in text generation and user interaction. One field benefiting greatly from these advancements is cybersecurity. In cybersecurity, many parameters that need to be protected and exchanged between senders and receivers are in the form of text and tabular data, making NLP a valuable tool in enhancing the security measures of communication protocols. This survey paper provides a comprehensive analysis of the utilization of Transformers andLLMsin cyber-threat detection systems. The methodology of paper selection and bibliometric analysis is outlined to establish a rigorous framework for evaluating existing research. The fundamentals of Transformers are discussed, including background information on various cyber-attacks and datasets commonly used in this field. The survey explores the application of Transformers in IDSs, focusing on different architectures such as Attention-basedmodels,LLMslike BERT andGPT, CNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others. Furthermore, it explores the diverse environments and applications where Transformers andLLMs-based IDS have been implemented, including computer networks, IoT devices, critical infrastructure protection, cloud computing, SDN, as well as in autonomous vehicles. The paper also addresses research challenges and future directions in this area, identifying key issues such as interpretability, scalability, and adaptability to evolving threats, and more. Finally, the conclusion summarizes the findings and highlights the significance of Transformers andLLMsin enhancing cyber-threat detection capabilities, while also outlining potential avenues for further research and development.

### Assessing the Role of Lexical Semantics in Cross-lingual Transfer through Controlled Manipulations
- **标题**: 通过受控操作评估词汇语义在跨语言迁移中的作用
- **Categories**: cs.CL
- **摘要**: 虽然跨语言模型迁移在许多情况下都很有效，但对其工作条件的了解仍然有限。在本文中，我们重点评估词汇语义在跨语言迁移中的作用，并将其影响与其他语言属性的影响进行比较。通过单独检查每种语言的属性，我们系统地分析了英语和目标语言之间的差异如何影响将语言与英语预训练表示空间对齐的能力。我们通过模仿目标语言特定特征的方式人为地操纵英语句子，并报告每次操纵对表示空间对齐质量的影响。我们表明，虽然脚本或词序等属性对对齐质量的影响有限，但两种语言之间的词汇匹配程度（我们使用翻译熵的度量来定义）对其影响很大。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07599
- **Authors**: Roy Ilani,Taelin Karidi,Omri Abend
- **Abstract**: While cross-linguisticmodeltransfer is effective in many settings, there is still limited understanding of the conditions under which it works. In this paper, we focus on assessing the role of lexical semantics in cross-lingual transfer, as we compare its impact to that of otherlanguageproperties. Examining eachlanguageproperty individually, we systematically analyze how differences between English and a targetlanguageinfluence the capacity to align thelanguagewith an English pretrained representation space. We do so by artificially manipulating the English sentences in ways that mimic specific characteristics of the targetlanguage, and reporting the effect of each manipulation on the quality of alignment with the representation space. We show that while properties such as the script or word order only have a limited impact on alignment quality, the degree of lexical matching between the twolanguages, which we define using a measure of translation entropy, greatly affects it.

### WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation Integrating Web Search and Knowledge Graphs
- **标题**: WeKnow-RAG：一种集成网络搜索和知识图的检索增强生成自适应方法
- **Categories**: cs.CL cs.IR
- **摘要**: 大型语言模型（LLM）为自适应智能代理的发展做出了巨大贡献，并被定位为实现通用人工智能（AGI）的重要途径。然而，LLM很容易产生事实上不正确的信息，并且经常产生破坏其可靠性的“幻影”内容，这对其在现实场景中的部署构成了严峻的挑战。通过结合外部数据库和信息检索机制来增强LLMs是一条有效的途径。为了解决上述挑战，我们提出了一种名为 WeKnow-RAG 的新方法，它将网络搜索和知识图集成到“检索增强生成（RAG）”系统中。首先，通过将知识图的结构化表示与密集向量检索的灵活性相结合，提高了LLM响应的准确性和可靠性。然后，WeKnow-RAG 利用特定领域的知识图来满足各种查询和领域，从而通过采用稀疏和密集检索方法的多阶段网页检索技术来提高事实信息和复杂推理任务的性能。我们的方法有效地平衡了信息检索的效率和准确性，从而改善了整体检索过程。最后，我们还为法学硕士集成了自我评估机制，以评估其生成答案的可信度。我们的方法在广泛的离线实验和在线提交中证明了其卓越的有效性。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07611
- **Authors**: Weijian Xie,Xuefeng Liang,Yuhui Liu,Kaihua Ni,Hong Cheng,Zetian Hu
- **Abstract**: LargeLanguageModels(LLMs) have greatly contributed to the development of adaptive intelligent agents and are positioned as an important way to achieve Artificial General Intelligence (AGI). However,LLMsare prone to produce factually incorrect information and often produce "phantom" content that undermines their reliability, which poses a serious challenge for their deployment in real-world scenarios. EnhancingLLMsby combining external databases and information retrieval mechanisms is an effective path. To address the above challenges, we propose a new approach called WeKnow-RAG, which integrates Web search and Knowledge Graphs into a "Retrieval-Augmented Generation (RAG)" system. First, the accuracy and reliability ofLLMresponses are improved by combining the structured representation of Knowledge Graphs with the flexibility of dense vector retrieval. WeKnow-RAG then utilizes domain-specific knowledge graphs to satisfy a variety of queries and domains, thereby improving performance on factual information and complex reasoning tasks by employing multi-stage web page retrieval techniques using both sparse and dense retrieval methods. Our approach effectively balances the efficiency and accuracy of information retrieval, thus improving the overall retrieval process. Finally, we also integrate a self-assessment mechanism for theLLMto evaluate the trustworthiness of the answers it generates. Our approach proves its outstanding effectiveness in a wide range of offline experiments and online submissions.

### Hierarchical Working Memory and a New Magic Number
- **标题**: 分层工作记忆和新的神奇数字
- **Categories**: q-bio.NC cond-mat.dis-nn cs.CL
- **摘要**: 极其有限的工作记忆范围（通常约为四个项目）与我们同时处理大量感官信息流的日常经验形成鲜明对比。这种差异表明工作记忆可以将信息组织成紧凑的表示形式，例如块，但潜在的神经机制仍然很大程度上未知。在这里，我们提出了一种在工作记忆突触理论框架内进行分块的循环神经网络模型。我们表明，通过选择性地抑制刺激组，网络可以以块的形式维护和检索刺激，从而超出了基本能力。此外，我们表明我们的模型可以通过分层分块在工作记忆中动态构建分层表示。这种提出的机制的结果是对可以存储和随后从工作存储器中检索的项目数量的新限制，仅取决于未调用分块时的基本工作存储器容量。通过分析癫痫患者的单一反应和言语材料的记忆实验，证实了我们模型的预测。我们的工作提供了一个新颖的概念和分析框架，用于理解大脑中对认知至关重要的信息的动态组织。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07637
- **Authors**: Weishun Zhong,Mikhail Katkov,Misha Tsodyks
- **Abstract**: The extremely limited working memory span, typically around four items, contrasts sharply with our everyday experience of processing much larger streams of sensory information concurrently. This disparity suggests that working memory can organize information into compact representations such as chunks, yet the underlying neural mechanisms remain largely unknown. Here, we propose a recurrent neural networkmodelfor chunking within the framework of the synaptic theory of working memory. We showed that by selectively suppressing groups of stimuli, the network can maintain and retrieve the stimuli in chunks, hence exceeding the basic capacity. Moreover, we show that ourmodelcan dynamically construct hierarchical representations within working memory through hierarchical chunking. A consequence of this proposed mechanism is a new limit on the number of items that can be stored and subsequently retrieved from working memory, depending only on the basic working memory capacity when chunking is not invoked. Predictions from ourmodelwere confirmed by analyzing single-unit responses in epileptic patients and memory experiments with verbal material. Our work provides a novel conceptual and analytical framework for understanding the on-the-fly organization of information in the brain that is crucial for cognition.

### Alignment-Enhanced Decoding:Defending via Token-Level Adaptive Refining of Probability Distributions
- **标题**: 对齐增强解码：通过概率分布的令牌级自适应细化进行防御
- **Categories**: cs.CL cs.AI
- **摘要**: 大型语言模型容易受到越狱攻击，这可能导致有害内容的生成。虽然先前的防御措施通过干扰或检查输入来减轻这些风险，但它们忽略了竞争目标，即对齐失败的根本原因。在本文中，我们提出了对齐增强解码（AED），这是一种采用自适应解码来解决越狱问题的根本原因的新颖防御方法。我们首先定义竞争指数来量化对齐失败，并利用自我评估的反馈来计算对齐后的逻辑。然后，AED 自适应地将 AED 和对齐后的 logits 与原始 logits 结合起来，以获得无害且有用的分布。因此，我们的方法增强了安全性，同时保持了有用性。我们对五种模型和四种常见越狱进行了实验，结果验证了我们方法的有效性。代码可在 https://github.com/GIGABaozi/AED.git 获取。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07663
- **Authors**: Quan Liu,Zhenhong Zhou,Longzhu He,Yi Liu,Wei Zhang,Sen Su
- **Abstract**: Largelanguagemodelsare susceptible to jailbreak attacks, which can result in the generation of harmful content. While prior defenses mitigate these risks by perturbing or inspecting inputs, they ignore competing objectives, the underlying cause of alignment failures. In this paper, we propose Alignment-Enhanced Decoding (AED), a novel defense that employs adaptive decoding to address the root causes of jailbreak issues. We first define the Competitive Index to quantify alignment failures and utilize feedback from self-evaluation to compute post-alignment logits. Then, AED adaptively combines AED and post-alignment logits with the original logits to obtain harmless and helpful distributions. Consequently, our method enhances safety alignment while maintaining helpfulness. We conduct experiments across fivemodelsand four common jailbreaks, with the results validating the effectiveness of our approach. Code is available at https://github.com/GIGABaozi/AED.git.

### Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech LargeLanguageModels
- **标题**: Spoken Stereoset：评估语音大语言模型中对说话者的社会偏见
- **Categories**: cs.CL eess.AS
- **摘要**: 警告：本文可能包含令人不舒服的内容。
  大型语言模型 (LLM) 在各种任务中都取得了显着的性能，包括涉及语音等多模态数据的任务。然而，由于训练数据的性质，这些模型经常表现出偏差。最近，更多的语音大型语言模型（SLLM）的出现，强调了解决这些偏见的迫切需要。本研究引入了 Spoken Stereoset，这是一个专门为评估 SLLM 中的社会偏见而设计的数据集。通过研究不同模型如何响应不同人口群体的言论，我们的目标是识别这些偏见。我们的实验揭示了对他们的表现和偏见水平的重要见解。研究结果表明，虽然大多数模型表现出最小的偏见，但有些模型仍然表现出轻微的刻板或反刻板倾向。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07665
- **Authors**: Yi-Cheng Lin,Wei-Chih Chen,Hung-yi Lee
- **Abstract**: Warning: This paper may contain texts with uncomfortable content.
  LargeLanguageModels(LLMs) have achieved remarkable performance in various tasks, including those involvingmultimodaldata like speech. However, thesemodelsoften exhibit biases due to the nature of their training data. Recently, more Speech LargeLanguageModels(SLLMs) have emerged, underscoring the urgent need to address these biases. This study introduces Spoken Stereoset, a dataset specifically designed to evaluate social biases in SLLMs. By examining how differentmodelsrespond to speech from diverse demographic groups, we aim to identify these biases. Our experiments reveal significant insights into their performance and bias levels. The findings indicate that while mostmodelsshow minimal bias, some still exhibit slightly stereotypical or anti-stereotypical tendencies.

### ModelMerging inLLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities
- **标题**: 法学硕士、多层硕士及其他领域的模型合并：方法、理论、应用和机会
- **Categories**: cs.LG cs.AI cs.CL cs.CV
- **摘要**: 模型合并是机器学习社区中一种高效的赋能技术，不需要收集原始训练数据，也不需要昂贵的计算。随着模型合并在各个领域变得越来越普遍，全面了解可用的模型合并技术至关重要。然而，文献中对于这些技术的系统和彻底的回顾还存在很大的差距。本综述全面概述了模型合并方法和理论、它们在各个领域和环境中的应用以及未来的研究方向。具体来说，我们首先提出了一种新的分类方法，详尽地讨论了现有的模型合并方法。其次，我们讨论了模型合并技术在大语言模型、多模态大语言模型和10多个机器学习子领域中的应用，包括持续学习、多任务学习、小样本学习等。最后，我们强调了模型合并的剩余挑战并讨论了未来的研究方向。有关模型合并的论文的完整列表可在 \url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications} 获取。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07666
- **Authors**: Enneng Yang,Li Shen,Guibing Guo,Xingwei Wang,Xiaochun Cao,Jie Zhang,Dacheng Tao
- **Abstract**: Modelmerging is an efficient empowerment technique in the machine learning community that does not require the collection of raw training data and does not require expensive computation. Asmodelmerging becomes increasingly prevalent across various fields, it is crucial to understand the availablemodelmerging techniques comprehensively. However, there is a significant gap in the literature regarding a systematic and thorough review of these techniques. This survey provides a comprehensive overview ofmodelmerging methods and theories, their applications in various domains and settings, and future research directions. Specifically, we first propose a new taxonomic approach that exhaustively discusses existingmodelmerging methods. Secondly, we discuss the application ofmodelmerging techniques in largelanguagemodels,multimodallargelanguagemodels, and 10+ machine learning subfields, including continual learning, multi-task learning, few-shot learning, etc. Finally, we highlight the remaining challenges ofmodelmerging and discuss future research directions. A comprehensive list of papers aboutmodelmerging is available at \url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.

### End-to-end Semantic-centric Video-basedMultimodalAffective Computing
- **标题**: 端到端以语义为中心的基于视频的多模态情感计算
- **Categories**: cs.CV cs.AI cs.LG cs.MM
- **摘要**: 在通往通用人工智能（AGI）的道路上，理解人类的情感对于增强机器的认知能力至关重要。为了实现更感性的人机交互，人声视频中的多模态情感计算（MAC）引起了越来越多的关注。然而，以前的方法主要致力于设计多模态融合算法，存在两个问题：不同的预处理操作导致的语义不平衡以及不同模态中包含的情感内容与多模态真实情况不一致而引起的语义不匹配。此外，手动特征提取器的使用使得它们无法为多个 MAC 下游任务构建端到端管道。为了解决上述挑战，我们提出了一种名为 SemanticMAC 的新颖的端到端框架来计算对人类口语视频的多模态以语义为中心的影响。我们首先在多模态数据预处理中采用预训练的 Transformer 模型，并设计情感感知器模块来捕获单模态情感信息。此外，我们提出了一种以语义为中心的方法，以三种方式统一多模态表示学习，包括门控特征交互、多任务伪标签生成和样本内/样本间对比学习。最后，SemanticMAC 在以语义为中心的标签的指导下有效地学习特定和共享语义表示。大量的实验结果表明，我们的方法在四个 MAC 下游任务中的 7 个公共数据集上超越了最先进的方法。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07694
- **Authors**: Ronghao Lin,Ying Zeng,Sijie Mai,Haifeng Hu
- **Abstract**: In the pathway toward Artificial General Intelligence (AGI), understanding human's affection is essential to enhance machine's cognition abilities. For achieving more sensual human-AI interaction,MultimodalAffective Computing (MAC) in human-spoken videos has attracted increasing attention. However, previous methods are mainly devoted to designingmultimodalfusion algorithms, suffering from two issues: semantic imbalance caused by diverse pre-processing operations and semantic mismatch raised by inconsistent affection content contained in different modalities comparing with themultimodalground truth. Besides, the usage of manual features extractors make they fail in building end-to-end pipeline for multiple MAC downstream tasks. To address above challenges, we propose a novel end-to-end framework named SemanticMAC to computemultimodalsemantic-centric affection for human-spoken videos. We firstly employ pre-trained Transformermodelinmultimodaldata pre-processing and design Affective Perceiver module to capture unimodal affective information. Moreover, we present a semantic-centric approach to unifymultimodalrepresentation learning in three ways, including gated feature interaction, multi-task pseudo label generation, and intra-/inter-sample contrastive learning. Finally, SemanticMAC effectively learn specific- and shared-semantic representations in the guidance of semantic-centric labels. Extensive experimental results demonstrate that our approach surpass the state-of-the-art methods on 7 public datasets in four MAC downstream tasks.

### The Death of Schema Linking? Text-to-SQL in the Age of Well-ReasonedLanguageModels
- **标题**: 模式链接的消亡？合理语言模型时代的文本到 SQL
- **Categories**: cs.CL
- **摘要**: 模式链接是文本到 SQL 管道中的关键步骤，它将自然语言查询转换为 SQL。模式链接的目标是检索相关的表和列（信号），同时忽略不相关的表和列（噪声）。然而，不完美的模式链接通常会排除准确查询生成所需的基本列。在这项工作中，我们重新审视使​​用最新一代大型语言模型 (LLM) 时对模式链接的需求。我们根据经验发现，较新的模型擅长在生成过程中识别相关模式元素，而不需要显式模式链接。这允许文本到 SQL 管道完全绕过模式链接，而是将完整的数据库模式传递给 LLM，从而消除了排除必要信息的风险。此外，作为模式链接的替代方案，我们提出了在不影响基本模式信息的情况下提高文本到 SQL 准确性的技术。我们的方法在 BIRD 基准上实现了 71.83% 的执行准确率，在提交时排名第一。
- **Date**: 14 August, 2024
- **URL**: https://arxiv.org/abs/2408.07702
- **Authors**: Karime Maamari,Fadhil Abubaker,Daniel Jaroslawicz,Amine Mhedhbi
- **Abstract**: Schema linking is a crucial step in Text-to-SQL pipelines, which translate naturallanguagequeries into SQL. The goal of schema linking is to retrieve relevant tables and columns (signal) while disregarding irrelevant ones (noise). However, imperfect schema linking can often exclude essential columns needed for accurate query generation. In this work, we revisit the need for schema linking when using the latest generation of largelanguagemodels(LLMs). We find empirically that newermodelsare adept at identifying relevant schema elements during generation, without the need for explicit schema linking. This allows Text-to-SQL pipelines to bypass schema linking entirely and instead pass the full database schema to theLLM, eliminating the risk of excluding necessary information. Furthermore, as alternatives to schema linking, we propose techniques that improve Text-to-SQL accuracy without compromising on essential schema information. Our approach achieves 71.83\% execution accuracy on the BIRD benchmark, ranking first at the time of submission.

